{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GKE - flask - Let's Encrypt HTTPS\n",
    "#### Deploying a flask app on a K8s Cluster in the Google Cloud and access it through HTTPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wgiersche/workspace/tutorials/microblog\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gcloud container clusters create microblog-cluster --num-nodes=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Links: \n",
    "\n",
    "#### 1) [The flask mega-tutorial](https://blog.miguelgrinberg.com/)\n",
    "#### 2) [HTTPS on GKE](https://medium.com/a-little-bit/https-and-gke-6a0dc702603f)\n",
    "#### 3) [Install gcloud sdk](https://cloud.google.com/sdk/docs/quickstart-linux)\n",
    "#### 4) [Cluster access with kubectl](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)\n",
    "#### 5) [Install docker on Debian](https://docs.docker.com/install/linux/docker-ce/debian/#install-docker-ce-1)\n",
    "#### 6) [Viacheslav's Ready-to-go Approach for machine learning on GCE](https://blog.kovalevskyi.com/semi-managed-jupyter-lab-with-access-to-google-cloud-resources-cc6f9e439416)\n",
    "#### 7) [Pulling and Pushing Images](https://cloud.google.com/container-registry/docs/pushing-and-pulling)\n",
    "#### 8) [Sandeep's Best Practices for K8s](https://medium.com/google-cloud/kubernetes-best-practices-season-one-11119aee1d10)\n",
    "#### 9) []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow [the gcloud sdk installation guide](https://cloud.google.com/sdk/docs/quickstart-linux) to install the gcloud SDK. Clean up older executables when prompted to do so.\n",
    "\n",
    "Perform the following to install the gcloud, kubectl and docker \n",
    "```\n",
    "# Cloud SDK (gcloud and friends)\n",
    "export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\"\n",
    "echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "sudo apt-get update && sudo apt-get install -y google-cloud-sdk\n",
    "sudo apt-get install -y kubectl\n",
    "\n",
    "# docker\n",
    "sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common\n",
    "curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -\n",
    "sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable\"\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y docker-ce\n",
    "\n",
    "# Not sure whether this is really needed. Change to your service account.\n",
    "# Use gcloud auth list to identify your service account\n",
    "SERVICE_ACCOUNT=serviceAccount:1094881674505-compute@developer.gserviceaccount.com\n",
    "gsutil iam ch $SERVICE_ACCOUNT:admin gs://going-tfx\n",
    "\n",
    "# docker credential helper provides access to the image repo\n",
    "VERSION=1.5.0\n",
    "OS=linux\n",
    "ARCH=amd64\n",
    "curl -fsSL \"https://github.com/GoogleCloudPlatform/docker-credential-gcr/releases/download/v${VERSION}/docker-credential-gcr_${OS}_${ARCH}-${VERSION}.tar.gz\" | tar xz\n",
    "sudo mv docker-credential-gcr /usr/bin/docker-credential-gcr\n",
    "sudo chmod +x /usr/bin/docker-credential-gcr\n",
    "docker-credential-gcr\n",
    "```\n",
    "The last command should indicate that your active account is a service account, created for you on the fly to access the GCP resources.\n",
    "\n",
    "Now the following command should succeed and create a small 2-node cluster for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/kubectl\n",
      "/usr/bin/gcloud\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "which kubectl\n",
    "which gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Starting in 1.12, new clusters will have basic authentication disabled by default. Basic authentication can be enabled (or disabled) manually using the `--[no-]enable-basic-auth` flag.\n",
      "\u001b[1;33mWARNING:\u001b[0m Starting in 1.12, new clusters will not have a client certificate issued. You can manually enable (or disable) the issuance of the client certificate using the `--[no-]issue-client-certificate` flag.\n",
      "\u001b[1;33mWARNING:\u001b[0m Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n",
      "\u001b[1;33mWARNING:\u001b[0m Starting in 1.12, default node pools in new clusters will have their legacy Compute Engine instance metadata endpoints disabled by default. To create a cluster with legacy instance metadata endpoints disabled in the default node pool, run `clusters create` with the flag `--metadata disable-legacy-endpoints=true`.\n",
      "This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs.\n",
      "\u001b[1;33mWARNING:\u001b[0m Starting in Kubernetes v1.10, new clusters will no longer get compute-rw and storage-ro scopes added to what is specified in --scopes (though the latter will remain included in the default --scopes). To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set container/new_scopes_behavior true).\n",
      "Creating cluster microblog-cluster in us-central1-a...done.\n",
      "Created [https://container.googleapis.com/v1/projects/going-tfx/zones/us-central1-a/clusters/microblog-cluster].\n",
      "To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1-a/microblog-cluster?project=going-tfx\n",
      "kubeconfig entry generated for microblog-cluster.\n",
      "NAME               LOCATION       MASTER_VERSION  MASTER_IP      MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS\n",
      "microblog-cluster  us-central1-a  1.10.9-gke.5    35.239.25.105  n1-standard-1  1.10.9-gke.5  2          RUNNING\n"
     ]
    }
   ],
   "source": [
    "!gcloud container clusters create microblog-cluster --num-nodes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for microblog-cluster.\n"
     ]
    }
   ],
   "source": [
    "!gcloud container clusters get-credentials microblog-cluster --zone us-central1-a --project going-tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/kubernetes-dashboard-certs created\n",
      "serviceaccount/kubernetes-dashboard created\n",
      "deployment.apps/kubernetes-dashboard created\n",
      "service/kubernetes-dashboard created\n",
      "Error from server (Forbidden): error when creating \"https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\": roles.rbac.authorization.k8s.io is forbidden: User \"113438764705426694571\" cannot create roles.rbac.authorization.k8s.io in the namespace \"kube-system\": Required \"container.roles.create\" permission.\n",
      "Error from server (Forbidden): error when creating \"https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\": rolebindings.rbac.authorization.k8s.io is forbidden: User \"113438764705426694571\" cannot create rolebindings.rbac.authorization.k8s.io in the namespace \"kube-system\": Required \"container.roleBindings.create\" permission.\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl run microblog --image=us.gcr.io/going-tfx/microblog:v1 --port=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  7236  100  7236    0     0   113k      0 --:--:-- --:--:-- --:--:--  112k\n",
      "Helm v2.12.0 is already latest\n",
      "Run 'helm init' to configure helm.\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HELM_HOME has been configured at /home/wgiersche/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "Happy Helming!\n"
     ]
    }
   ],
   "source": [
    "!helm init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/helm\n"
     ]
    }
   ],
   "source": [
    "!which helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): deployments.extensions \"tiller-deploy\" not found\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"tiller\" deleted\n",
      "serviceaccount \"tiller\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kube-system delete deployment tiller-deploy\n",
    "!kubectl delete clusterrolebinding tiller\n",
    "!kubectl -n kube-system delete serviceaccount tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/tiller created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/tiller created\n",
      "$HELM_HOME has been configured at /home/wgiersche/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "Happy Helming!\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kube-system create serviceaccount tiller\n",
    "!kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\n",
    "!helm init --service-account=tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!helm ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: release cert-manager failed: customresourcedefinitions.apiextensions.k8s.io \"certificates.certmanager.k8s.io\" already exists\n"
     ]
    }
   ],
   "source": [
    "!helm install --name cert-manager --namespace cert-manager stable/cert-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"cert-manager\" deleted\n"
     ]
    }
   ],
   "source": [
    "!helm del --purge cert-manager;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
