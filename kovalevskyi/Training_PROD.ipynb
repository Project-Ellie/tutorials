{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import google.datalab.bigquery as dlbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print (\"Built with cuda\")\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### From Jupyter notebook to python package\n",
    "From exploration to production.\n",
    "\n",
    "This little tool dumps a given function to a file with the same name in a certain package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./train\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Data\n",
    "Training and evaluation data should be provided in files already.\n",
    "\n",
    "If not, please go back an run ```Processing_ATL_JUNE.ipynb```\n",
    "\n",
    "#### Fetch a sample file for examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://going-tfx/samples/train_data/atl_june_csv-00000-of-00024...\n",
      "/ [1 files][141.4 KiB/141.4 KiB]                                                \n",
      "Operation completed over 1 objects/141.4 KiB.                                    \n",
      "\n",
      "1000 records in /tmp/atl_june/samples/atl_june_csv-00000-of-00024\n"
     ]
    }
   ],
   "source": [
    "DATASET='samples'\n",
    "a_training_file = !gsutil ls gs://going-tfx/$DATASET/train_data/atl_june_csv-00000-of-*\n",
    "a_training_file = a_training_file[0]\n",
    "TEMP_DIR='/tmp/atl_june/{}'.format(DATASET)\n",
    "!rm -rf $TEMP_DIR\n",
    "!mkdir -p $TEMP_DIR\n",
    "!gsutil cp $a_training_file $TEMP_DIR\n",
    "a_training_file = !ls $TEMP_DIR\n",
    "a_training_file = os.path.join(TEMP_DIR,a_training_file[0])\n",
    "res=!wc -l $a_training_file\n",
    "res=res[0].split(\" \")\n",
    "print()\n",
    "print(\"{} records in {}\".format(res[0], res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look into the first training data file\n",
    "\n",
    "This data is at the **training data** stage. It's got all and only the columns we want. Is has been normalized and integerized. We'll use the ```tf.feature_column``` API to further prepare categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_HOD</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DIFF_LAT</th>\n",
       "      <th>DIFF_LON</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>38.94</td>\n",
       "      <td>-77.46</td>\n",
       "      <td>0.078493</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.488613</td>\n",
       "      <td>0.864046</td>\n",
       "      <td>0.102870</td>\n",
       "      <td>0.570397</td>\n",
       "      <td>0.592437</td>\n",
       "      <td>0.408602</td>\n",
       "      <td>0.59375</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.372093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>40.69</td>\n",
       "      <td>-74.16</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.528870</td>\n",
       "      <td>0.899484</td>\n",
       "      <td>0.150884</td>\n",
       "      <td>0.391697</td>\n",
       "      <td>0.760504</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.012501</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AIRLINE  ARR  ARR_DELAY  ARR_LAT  ARR_LON  DEP_DELAY  DEP_DOW  DEP_HOD  \\\n",
       "335        0   13       -6.0    38.94   -77.46   0.078493        2       18   \n",
       "560        2    3      -13.0    40.69   -74.16   0.076923        6       11   \n",
       "\n",
       "     DEP_LAT  DEP_LON  DIFF_LAT  DIFF_LON  DISTANCE  MEAN_TEMP_ARR  \\\n",
       "335    33.63   -84.42  0.488613  0.864046  0.102870       0.570397   \n",
       "560    33.63   -84.42  0.528870  0.899484  0.150884       0.391697   \n",
       "\n",
       "     MEAN_TEMP_DEP  MEAN_VIS_ARR  MEAN_VIS_DEP  WND_SPD_ARR  WND_SPD_DEP  \n",
       "335       0.592437      0.408602       0.59375     0.005101     0.372093  \n",
       "560       0.760504      0.473118       1.00000     0.012501     0.263566  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "probe = pd.read_csv(a_training_file, names=ORDERED_TRAINING_COLUMNS)\n",
    "probe.sample(frac=1.0)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_HOD</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DIFF_LAT</th>\n",
       "      <th>DIFF_LON</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.367000</td>\n",
       "      <td>44.106000</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>35.45627</td>\n",
       "      <td>-86.816130</td>\n",
       "      <td>0.107582</td>\n",
       "      <td>3.696000</td>\n",
       "      <td>14.435000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.408472</td>\n",
       "      <td>0.763572</td>\n",
       "      <td>0.123891</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>0.556386</td>\n",
       "      <td>0.435118</td>\n",
       "      <td>0.700406</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.374411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.387889</td>\n",
       "      <td>36.831227</td>\n",
       "      <td>35.743373</td>\n",
       "      <td>5.25087</td>\n",
       "      <td>11.891074</td>\n",
       "      <td>0.052509</td>\n",
       "      <td>1.726431</td>\n",
       "      <td>4.623327</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120793</td>\n",
       "      <td>0.127696</td>\n",
       "      <td>0.104027</td>\n",
       "      <td>0.133286</td>\n",
       "      <td>0.234825</td>\n",
       "      <td>0.077546</td>\n",
       "      <td>0.281966</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.109097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-29.000000</td>\n",
       "      <td>18.33000</td>\n",
       "      <td>-157.920000</td>\n",
       "      <td>0.058085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153430</td>\n",
       "      <td>0.109244</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>31.32000</td>\n",
       "      <td>-90.500000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.313320</td>\n",
       "      <td>0.724012</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>0.431408</td>\n",
       "      <td>0.424370</td>\n",
       "      <td>0.408602</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.43000</td>\n",
       "      <td>-83.310000</td>\n",
       "      <td>0.083987</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.407867</td>\n",
       "      <td>0.801224</td>\n",
       "      <td>0.103682</td>\n",
       "      <td>0.546931</td>\n",
       "      <td>0.592437</td>\n",
       "      <td>0.470430</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.364341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>39.87000</td>\n",
       "      <td>-80.040000</td>\n",
       "      <td>0.117739</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.510007</td>\n",
       "      <td>0.836340</td>\n",
       "      <td>0.147357</td>\n",
       "      <td>0.615523</td>\n",
       "      <td>0.760504</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007401</td>\n",
       "      <td>0.449612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>61.17000</td>\n",
       "      <td>-64.970000</td>\n",
       "      <td>0.503925</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951263</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018602</td>\n",
       "      <td>0.612403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AIRLINE          ARR    ARR_DELAY     ARR_LAT      ARR_LON  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.00000  1000.000000   \n",
       "mean      1.367000    44.106000    17.270000    35.45627   -86.816130   \n",
       "std       2.387889    36.831227    35.743373     5.25087    11.891074   \n",
       "min       0.000000     0.000000   -29.000000    18.33000  -157.920000   \n",
       "25%       0.000000    12.750000    -5.000000    31.32000   -90.500000   \n",
       "50%       1.000000    35.000000     6.000000    35.43000   -83.310000   \n",
       "75%       2.000000    68.000000    28.250000    39.87000   -80.040000   \n",
       "max      18.000000   169.000000   267.000000    61.17000   -64.970000   \n",
       "\n",
       "         DEP_DELAY      DEP_DOW      DEP_HOD  DEP_LAT  DEP_LON     DIFF_LAT  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.00  1000.00  1000.000000   \n",
       "mean      0.107582     3.696000    14.435000    33.63   -84.42     0.408472   \n",
       "std       0.052509     1.726431     4.623327     0.00     0.00     0.120793   \n",
       "min       0.058085     1.000000     6.000000    33.63   -84.42     0.014493   \n",
       "25%       0.076923     2.000000    10.000000    33.63   -84.42     0.313320   \n",
       "50%       0.083987     4.000000    14.000000    33.63   -84.42     0.407867   \n",
       "75%       0.117739     6.000000    18.000000    33.63   -84.42     0.510007   \n",
       "max       0.503925     6.000000    23.000000    33.63   -84.42     1.000000   \n",
       "\n",
       "          DIFF_LON     DISTANCE  MEAN_TEMP_ARR  MEAN_TEMP_DEP  MEAN_VIS_ARR  \\\n",
       "count  1000.000000  1000.000000    1000.000000    1000.000000   1000.000000   \n",
       "mean      0.763572     0.123891       0.522581       0.556386      0.435118   \n",
       "std       0.127696     0.104027       0.133286       0.234825      0.077546   \n",
       "min       0.000000     0.000000       0.153430       0.109244      0.112903   \n",
       "25%       0.724012     0.059060       0.431408       0.424370      0.408602   \n",
       "50%       0.801224     0.103682       0.546931       0.592437      0.470430   \n",
       "75%       0.836340     0.147357       0.615523       0.760504      0.489247   \n",
       "max       0.998174     1.000000       0.951263       0.928571      0.806452   \n",
       "\n",
       "       MEAN_VIS_DEP  WND_SPD_ARR  WND_SPD_DEP  \n",
       "count   1000.000000  1000.000000  1000.000000  \n",
       "mean       0.700406     0.006080     0.374411  \n",
       "std        0.281966     0.002626     0.109097  \n",
       "min        0.000000     0.000200     0.263566  \n",
       "25%        0.593750     0.004200     0.263566  \n",
       "50%        0.687500     0.005701     0.364341  \n",
       "75%        1.000000     0.007401     0.449612  \n",
       "max        1.000000     0.018602     0.612403  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ```tf.data``` input function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These input functions read from any number of files containing pre-processed data\n",
    "In our parlour, this data is at the **training stage**. It's been fetched from a BigQuery table and pre-processed. It's not what'll come in at prediction time!\n",
    "\n",
    "That pre-processing function has been stored in a metadata directory that is available to us to treat data at prediction time (**signature stage**) exactly the same way that our training data has been treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from TFRecords\n",
    "This is highly optimized for throughput, but files are not that easy to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_tfr_input_fn written to ./train/make_tfr_input_fn.py.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tfr_input_fn(filename_pattern, batch_size, shuffle_buffer_size=10000, distribute=False,\n",
    "                     reader_num_threads=16, parser_num_threads=16, sloppy_ordering=True,\n",
    "                      prefetch_buffer_size=1024):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from train.model_config import LABEL_COLUMN\n",
    "    from train.model_config import TRAINING_METADATA\n",
    "\n",
    "    feature_spec = TRAINING_METADATA.schema.as_feature_spec()\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=filename_pattern,\n",
    "            batch_size=batch_size,\n",
    "            features=feature_spec,\n",
    "            shuffle_buffer_size=shuffle_buffer_size,\n",
    "            prefetch_buffer_size=prefetch_buffer_size,\n",
    "            reader_num_threads=reader_num_threads,\n",
    "            parser_num_threads=parser_num_threads,\n",
    "            sloppy_ordering=sloppy_ordering,\n",
    "            label_key=LABEL_COLUMN)\n",
    "\n",
    "        if distribute:\n",
    "            return dataset \n",
    "        else:\n",
    "            return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_tfr_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Verify the input_function's behaviour\n",
    "```make_input_fn()``` returns an input_function. This function will be passed to the estimator, such that the estimator can call it in its own session/graph context to create a particularly useful input tensor. That input tensor will return the next batch of input records whenever it is evaluated. Let's create that tensor ourselves to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'AIRLINE': array([1, 2]),\n",
       "   'ARR': array([83, 53]),\n",
       "   'ARR_LAT': array([40.47, 42.96], dtype=float32),\n",
       "   'ARR_LON': array([-88.91, -83.74], dtype=float32),\n",
       "   'DEP_DELAY': array([0.07378336, 0.07535322], dtype=float32),\n",
       "   'DEP_DOW': array([6, 5]),\n",
       "   'DEP_HOD': array([17,  8]),\n",
       "   'DEP_LAT': array([33.63, 33.63], dtype=float32),\n",
       "   'DEP_LON': array([-84.42, -84.42], dtype=float32),\n",
       "   'DIFF_LAT': array([0.52380955, 0.5810904 ], dtype=float32),\n",
       "   'DIFF_LON': array([0.7410867 , 0.79660654], dtype=float32),\n",
       "   'DISTANCE': array([0.10285335, 0.12829283], dtype=float32),\n",
       "   'MEAN_TEMP_ARR': array([0.54512626, 0.31588444], dtype=float32),\n",
       "   'MEAN_TEMP_DEP': array([0.75210077, 0.605042  ], dtype=float32),\n",
       "   'MEAN_VIS_ARR': array([0.48387095, 0.4784946 ], dtype=float32),\n",
       "   'MEAN_VIS_DEP': array([1.   , 0.875], dtype=float32),\n",
       "   'WND_SPD_ARR': array([0.01020102, 0.00260026], dtype=float32),\n",
       "   'WND_SPD_DEP': array([0.39534882, 0.2868217 ], dtype=float32)},\n",
       "  array([-27.,   1.], dtype=float32))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_input_fn = make_tfr_input_fn(\n",
    "        \"gs://going-tfx/{}/train_data/atl_june_tfr-00000-of-00024\".format(DATASET), batch_size=2)\n",
    "    input = train_input_fn()\n",
    "    res = [sess.run(input) for i in range(1)]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Read from CSV\n",
    "Not so good for production, but may come handy for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_input_fn written to ./train/make_input_fn.py.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_input_fn(filename_pattern, batch_size, shuffle_buffer_size=None, distribute=False):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from train.model_config import ORDERED_TRAINING_DEFAULTS\n",
    "    from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "    from train.model_config import LABEL_COLUMN\n",
    "    \n",
    "    \n",
    "    def _input_fn():\n",
    "        filenames = tf.gfile.Glob(filename_pattern)\n",
    "        dataset = tf.data.TextLineDataset(filenames)\n",
    "\n",
    "        def decode_csv(row):\n",
    "            cols = tf.decode_csv(row, record_defaults=ORDERED_TRAINING_DEFAULTS)\n",
    "            features = dict(zip(ORDERED_TRAINING_COLUMNS, cols))\n",
    "            return features\n",
    "\n",
    "        def pop_target(features):\n",
    "            target = features.pop(LABEL_COLUMN)\n",
    "            return features, target\n",
    "        \n",
    "        if shuffle_buffer_size is not None:\n",
    "            dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "                \n",
    "        dataset = (dataset.repeat()\n",
    "                   .map(decode_csv)\n",
    "                   .map(pop_target)\n",
    "                   .batch(batch_size))\n",
    "        \n",
    "        if distribute:\n",
    "            return dataset \n",
    "        else:\n",
    "            return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'AIRLINE': array([5, 5], dtype=int32),\n",
       "   'ARR': array([58, 43], dtype=int32),\n",
       "   'ARR_LAT': array([30.19, 36.12], dtype=float32),\n",
       "   'ARR_LON': array([-97.67, -86.67], dtype=float32),\n",
       "   'DEP_DELAY': array([0.07692308, 0.10204082], dtype=float32),\n",
       "   'DEP_DOW': array([1, 5], dtype=int32),\n",
       "   'DEP_HOD': array([16, 17], dtype=int32),\n",
       "   'DEP_LAT': array([33.63, 33.63], dtype=float32),\n",
       "   'DEP_LON': array([-84.42, -84.42], dtype=float32),\n",
       "   'DIFF_LAT': array([0.2873246 , 0.42374048], dtype=float32),\n",
       "   'DIFF_LON': array([0.6470146, 0.7651418], dtype=float32),\n",
       "   'DISTANCE': array([0.16594191, 0.03058177], dtype=float32),\n",
       "   'MEAN_TEMP_ARR': array([0.7635379 , 0.54512626], dtype=float32),\n",
       "   'MEAN_TEMP_DEP': array([0.91176444, 0.75210077], dtype=float32),\n",
       "   'MEAN_VIS_ARR': array([0.4892473 , 0.45698923], dtype=float32),\n",
       "   'MEAN_VIS_DEP': array([0.70312506, 1.        ], dtype=float32),\n",
       "   'WND_SPD_ARR': array([0.00580058, 0.00330033], dtype=float32),\n",
       "   'WND_SPD_DEP': array([0.49612403, 0.36434108], dtype=float32)},\n",
       "  array([-4., 47.], dtype=float32))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_input_fn = make_input_fn(\n",
    "        \"gs://going-tfx/{}/eval_data/*\".format(DATASET), batch_size=2)\n",
    "    input = train_input_fn()\n",
    "    res = [sess.run(input) for i in range(1)]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated the input tensor twice to each time receive a batch of 2 examples from the list of input files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Additional feature engineering for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns need to be treated once more to derive at numerical input suitable for model training. That involves bucketizing, the use of dictionaries, feature crossing and embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find ranges to bucketize latitude and longitude \n",
    "We can easily understand the range of values with the help of a bq query and ```pandas.describe()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>344.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.491570</td>\n",
       "      <td>-98.531599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.547964</td>\n",
       "      <td>21.746974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.480000</td>\n",
       "      <td>-176.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.450000</td>\n",
       "      <td>-111.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.715000</td>\n",
       "      <td>-93.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.907500</td>\n",
       "      <td>-82.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.280000</td>\n",
       "      <td>-64.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lat         lon\n",
       "count  344.000000  344.000000\n",
       "mean    38.491570  -98.531599\n",
       "std      8.547964   21.746974\n",
       "min     13.480000 -176.640000\n",
       "25%     33.450000 -111.675000\n",
       "50%     38.715000  -93.300000\n",
       "75%     42.907500  -82.497500\n",
       "max     71.280000  -64.800000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "select \n",
    "    distinct arrival_airport as airport, arrival_lat as lat, arrival_lon as lon \n",
    "from \n",
    "    `bigquery-samples.airline_ontime_data.flights`\n",
    "\"\"\"\n",
    "locations = dlbq.Query(query).execute().result().to_dataframe()\n",
    "locations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_boundaries = range(10,80,5)\n",
    "lat_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, -95, -90, -85, -80, -75, -70, -65, -60]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_boundaries = range(-100, -55, 5)\n",
    "lon_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use those boundaries in the function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tf feature_column api for bucketizing, crossing and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'create_feature_columns written to ./train/create_feature_columns.py.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_feature_columns():\n",
    "    \n",
    "    from tensorflow.feature_column import indicator_column as ind\n",
    "    from tensorflow.feature_column import numeric_column as num\n",
    "    from tensorflow.feature_column import bucketized_column as buck\n",
    "    from tensorflow.feature_column import crossed_column as cross\n",
    "    from tensorflow.feature_column import embedding_column as emb\n",
    "    from tensorflow.feature_column import categorical_column_with_identity as cid\n",
    "    \n",
    "    ################################################################\n",
    "    #  Numerical columns for the pre-processed features\n",
    "    ################################################################\n",
    "    feature_columns = [\n",
    "        num(col) for col in [\n",
    "            'DEP_DELAY',  \n",
    "            'MEAN_TEMP_DEP','MEAN_VIS_DEP','WND_SPD_DEP',\n",
    "            'MEAN_TEMP_ARR','MEAN_VIS_ARR','WND_SPD_ARR',\n",
    "            'DIFF_LAT','DIFF_LON','DISTANCE']]\n",
    "    \n",
    "    ################################################################\n",
    "    #  categorical from ints, bucket counts from examination of the \n",
    "    #  full dataset\n",
    "    ################################################################\n",
    "    airline = ind(cid('AIRLINE', num_buckets=30))\n",
    "    arrival = ind(cid('ARR', num_buckets=400))\n",
    "    \n",
    "    ################################################################\n",
    "    #  Crossed and embedded\n",
    "    ################################################################\n",
    "    lat_boundaries = range(10,80,5)\n",
    "    lon_boundaries = range(-100, -55, 5)\n",
    "    cross_size = len(lat_boundaries) * len(lon_boundaries)\n",
    "\n",
    "    arr_geo_emb = emb(cross([\n",
    "        buck(num('ARR_LAT'), lat_boundaries), \n",
    "        buck(num('ARR_LON'), lon_boundaries)], cross_size), 10)\n",
    "\n",
    "    dep_geo_emb = emb(cross([\n",
    "        buck(num(\"DEP_LAT\"), lat_boundaries), \n",
    "        buck(num(\"DEP_LON\"), lon_boundaries)], cross_size), 10)\n",
    "\n",
    "    dep_how_emb = emb(cross([\n",
    "        cid(\"DEP_HOD\", num_buckets=24), \n",
    "        cid(\"DEP_DOW\", num_buckets=8)], 7*24), 10)\n",
    "\n",
    "    ################################################################\n",
    "    #  all together\n",
    "    ################################################################\n",
    "    return (\n",
    "        feature_columns + \n",
    "        [airline, arrival] +\n",
    "        [dep_how_emb, arr_geo_emb, dep_geo_emb])\n",
    "    \n",
    "write_py(create_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='DEP_DELAY', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='MEAN_TEMP_DEP', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='MEAN_VIS_DEP', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='WND_SPD_DEP', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='MEAN_TEMP_ARR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='MEAN_VIS_ARR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='WND_SPD_ARR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='DIFF_LAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='DIFF_LON', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='DISTANCE', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key='AIRLINE', num_buckets=30, default_value=None)),\n",
       " _IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key='ARR', num_buckets=400, default_value=None)),\n",
       " _EmbeddingColumn(categorical_column=_CrossedColumn(keys=(_IdentityCategoricalColumn(key='DEP_HOD', num_buckets=24, default_value=None), _IdentityCategoricalColumn(key='DEP_DOW', num_buckets=8, default_value=None)), hash_bucket_size=168, hash_key=None), dimension=10, combiner='mean', layer_creator=<function _creator at 0x7fa72322a050>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='ARR_LAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75)), _BucketizedColumn(source_column=_NumericColumn(key='ARR_LON', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-100, -95, -90, -85, -80, -75, -70, -65, -60))), hash_bucket_size=126, hash_key=None), dimension=10, combiner='mean', layer_creator=<function _creator at 0x7fa7425208c0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='DEP_LAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75)), _BucketizedColumn(source_column=_NumericColumn(key='DEP_LON', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-100, -95, -90, -85, -80, -75, -70, -65, -60))), hash_bucket_size=126, hash_key=None), dimension=10, combiner='mean', layer_creator=<function _creator at 0x7fa72322a5f0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature columns encode a construction plan. The ```tf.feature_column.input_column()``` helper will construct a sub-graph from this plan and feed the root (the *result*) of the graph into the model. You see the pattern: All parts of the tensor graph are created within the session/graph context of the ```Estimator``` API. Never outside of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hypothesis, model function and custom estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The hypothesis\n",
    "This computational sub-graph represents the trainable hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_hypothesis written to ./train/make_hypothesis.py.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_hypothesis(input_layer, options):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "        \n",
    "    out = tf.layers.dense(input_layer, 1, activation=None)\n",
    "\n",
    "    return out\n",
    "    \n",
    "write_py(make_hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model function\n",
    "The model function is responsible for providing different variants of the actual model suitable for training, evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_model_fn written to ./train/make_model_fn.py.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_model_fn(feature_columns, options):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from train.make_hypothesis import make_hypothesis\n",
    "    \n",
    "    def _model_fn(features, labels, mode):\n",
    "\n",
    "        input_layer = tf.feature_column.input_layer( \n",
    "            features, feature_columns=feature_columns)\n",
    "\n",
    "        #############################################################\n",
    "        # This single line is the actual model\n",
    "        #############################################################\n",
    "        out = make_hypothesis(input_layer, options)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=out)\n",
    "\n",
    "\n",
    "        labels = tf.expand_dims(labels, -1)\n",
    "        loss = tf.losses.mean_squared_error(labels, out)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:    \n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss = loss,\n",
    "                #eval_metric_ops={'my_metric': }\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(options['learning_rate'])\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(  \n",
    "                mode,\n",
    "                loss = loss,\n",
    "                train_op = train_op)\n",
    "        \n",
    "    return _model_fn\n",
    "write_py(make_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The tft serving input function\n",
    "The saved ```transform_fn``` of the preprocessing step is recovered and applied to the **signature** input, which is represented by placeholders.\n",
    "This function will be exported together with the ```tf.feature_columns``` transforms after training, to make sure that exactly the same preprocessing steps will be executed at prediction time.\n",
    "The estimator will apply this function to the signature data coming at prediction time. The result will then be feed into the graph constructed with the help of the ```feature_columns``` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_tft_serving_input_fn written to ./train/make_tft_serving_input_fn.py.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tft_serving_input_fn(metadata_dir):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_transform as tft\n",
    "    from train.model_config import SIGNATURE_INT_COLUMNS\n",
    "    from train.model_config import SIGNATURE_FLOAT_COLUMNS\n",
    "    from train.model_config import SIGNATURE_STR_COLUMNS\n",
    "        \n",
    "    def _input_fn():\n",
    "        # placeholders for all the raw inputs\n",
    "        placeholders = {\n",
    "            key: tf.placeholder(name = key, shape=[None], dtype=tf.int64)\n",
    "            for key in SIGNATURE_INT_COLUMNS\n",
    "        }\n",
    "        placeholders.update({\n",
    "            key: tf.placeholder(name = key, shape=[None], dtype=tf.float32)\n",
    "            for key in SIGNATURE_FLOAT_COLUMNS\n",
    "        })\n",
    "\n",
    "        placeholders.update({\n",
    "            key: tf.placeholder(name = key, shape=[None], dtype=tf.string)\n",
    "            for key in SIGNATURE_STR_COLUMNS\n",
    "        })\n",
    "\n",
    "        # transform using the saved model in transform_fn        \n",
    "        transform_output = tft.TFTransformOutput(transform_output_dir=metadata_dir)\n",
    "        features = transform_output.transform_raw_features(placeholders)\n",
    "            \n",
    "        return tf.estimator.export.ServingInputReceiver(features, placeholders)\n",
    "\n",
    "    return _input_fn\n",
    "write_py(make_tft_serving_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training and Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At last, start the training!\n",
    "First, we use the smaller sample data to verify the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_and_evaluate written to ./train/train_and_evaluate.py.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_and_evaluate(options):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from train.make_model_fn import make_model_fn\n",
    "    from train.make_tft_serving_input_fn import make_tft_serving_input_fn\n",
    "    from train.create_feature_columns import create_feature_columns\n",
    "\n",
    "    #from train.make_input_fn import make_input_fn\n",
    "    from train.make_tfr_input_fn import make_tfr_input_fn\n",
    "    \n",
    "    feature_columns = create_feature_columns()\n",
    "    \n",
    "    if options['distribute']:\n",
    "        print(\"#####################################################################\")\n",
    "        print(\"    Runnin in distibuted mode\")\n",
    "        print(\"#####################################################################\")\n",
    "        strategy=tf.contrib.distribute.MirroredStrategy()    \n",
    "        config = tf.estimator.RunConfig(model_dir=options['model_dir'], train_distribute=strategy)\n",
    "    else:\n",
    "        config = tf.estimator.RunConfig(model_dir=options['model_dir'])\n",
    "        \n",
    "\n",
    "    model_fn = make_model_fn(feature_columns, options)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "            config=config,\n",
    "            model_fn=model_fn)\n",
    "\n",
    "    exporter = tf.estimator.LatestExporter('exporter', \n",
    "                                           make_tft_serving_input_fn(options['metadata_dir']))\n",
    "\n",
    "    train_input_fn = make_tfr_input_fn(\n",
    "        options['train_data_pattern'], shuffle_buffer_size=80000, \n",
    "        batch_size=options['train_batch_size'], distribute=options['distribute'],\n",
    "        prefetch_buffer_size=options['prefetch_buffer_size'])\n",
    "\n",
    "    eval_input_fn = make_tfr_input_fn(\n",
    "        options['eval_data_pattern'], \n",
    "        batch_size=options['eval_batch_size'])  \n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=options['max_train_steps'])\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps = options['eval_batch_size'], exporters=exporter)\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "    \n",
    "write_py(train_and_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############################################################################################################\n",
      "using directory gs://going-tfx/samples/model to store the model. Remove the directory if you want to start from scratch\n",
      "###############################################################################################################\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa7231bb210>, '_model_dir': 'gs://going-tfx/samples/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/wgiersche/py2/local/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into gs://going-tfx/samples/model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1882.4365, step = 1\n",
      "INFO:tensorflow:global_step/sec: 89.6872\n",
      "INFO:tensorflow:loss = 1668.5448, step = 101 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.052\n",
      "INFO:tensorflow:loss = 1775.0104, step = 201 (0.777 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.122\n",
      "INFO:tensorflow:loss = 1213.1636, step = 301 (0.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.828\n",
      "INFO:tensorflow:loss = 2049.9966, step = 401 (0.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.6783\n",
      "INFO:tensorflow:loss = 1077.6223, step = 501 (1.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.396\n",
      "INFO:tensorflow:loss = 1502.0093, step = 601 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.953\n",
      "INFO:tensorflow:loss = 1727.0796, step = 701 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.819\n",
      "INFO:tensorflow:loss = 1656.7783, step = 801 (0.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.053\n",
      "INFO:tensorflow:loss = 1695.6323, step = 901 (0.829 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into gs://going-tfx/samples/model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-19-14:45:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://going-tfx/samples/model/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [51/512]\n",
      "INFO:tensorflow:Evaluation [102/512]\n",
      "INFO:tensorflow:Evaluation [153/512]\n",
      "INFO:tensorflow:Evaluation [204/512]\n",
      "INFO:tensorflow:Evaluation [255/512]\n",
      "INFO:tensorflow:Evaluation [306/512]\n",
      "INFO:tensorflow:Evaluation [357/512]\n",
      "INFO:tensorflow:Evaluation [408/512]\n",
      "INFO:tensorflow:Evaluation [459/512]\n",
      "INFO:tensorflow:Evaluation [510/512]\n",
      "INFO:tensorflow:Evaluation [512/512]\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-19-14:45:55\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1785.5237\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: gs://going-tfx/samples/model/model.ckpt-1000\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from gs://going-tfx/samples/model/model.ckpt-1000\n",
      "WARNING:tensorflow:From /home/wgiersche/py2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py:1044: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: gs://going-tfx/samples/model/export/exporter/temp-1542638758/assets\n",
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/model/export/exporter/temp-1542638758/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 1215.4277.\n"
     ]
    }
   ],
   "source": [
    "from train.train_tools import join_paths\n",
    "\n",
    "args={}\n",
    "args['base_dir']='gs://going-tfx/{}'.format(DATASET)\n",
    "args['metadata_dir']='metadata'\n",
    "args['train_data_pattern']='train_data/atl_june_tfr*'\n",
    "args['eval_data_pattern']='eval_data/atl_june_tfr*'\n",
    "args['train_batch_size']=512\n",
    "args['eval_batch_size']=512\n",
    "args['max_train_steps']=1000\n",
    "args['eval_steps']=10\n",
    "args['learning_rate']=1e-3\n",
    "args['model_dir']='model'\n",
    "args['prefetch_buffer_size']=10000\n",
    "args['distribute']=False\n",
    "\n",
    "model_dir = os.path.join(args['base_dir'], args['model_dir'])\n",
    "print(\"\\n###############################################################################################################\")\n",
    "print(\"using directory {} to store the model. Remove the directory if you want to start from scratch\".format(model_dir))\n",
    "print(\"###############################################################################################################\")\n",
    "!gsutil -m rm -rf $model_dir\n",
    "\n",
    "#\n",
    "# You can't distribute work from a jupyter notebook. \n",
    "#\n",
    "train_and_evaluate(join_paths(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Train with the full training set of ~300k records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.train_tools import join_paths\n",
    "\n",
    "args={}\n",
    "args['base_dir']='gs://going-tfx/full'\n",
    "\n",
    "# using a full path allows us to specify other locations\n",
    "args['metadata_dir']='gs://going-tfx/sample/metadata'\n",
    "\n",
    "args['train_data_pattern']='train_data/*'\n",
    "args['eval_data_pattern']='eval_data/*'\n",
    "args['train_batch_size']=512\n",
    "args['eval_batch_size']=512\n",
    "args['max_train_steps']=1000 # That's about 5 epochs\n",
    "args['eval_steps']=10\n",
    "args['learning_rate']=1e-3\n",
    "args['model_dir']='model'\n",
    "\n",
    "model_dir = os.path.join(args['base_dir'], args['model_dir'])\n",
    "print(\"\\n###############################################################################################################\")\n",
    "print(\"using directory {} to store the model. Remove the directory if you want to start from scratch\".format(model_dir))\n",
    "print(\"###############################################################################################################\")\n",
    "# !gsutil rm -rf $model_dir\n",
    "\n",
    "train_and_evaluate(create_feature_columns(), join_paths(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some test data. Now we need signature data, and that's what we have in Bigquery. Remember? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT\n",
      "        DEP_LAT, DEP_LON, DEP_DELAY, MEAN_TEMP_DEP, MEAN_VIS_DEP, WND_SPD_DEP, ARR_LAT, ARR_LON, ARR_DELAY, MEAN_TEMP_ARR, MEAN_VIS_ARR, WND_SPD_ARR, DEP_DOW, DEP_T, ARR, AIRLINE\n",
      "    FROM \n",
      "        `going-tfx.examples.ATL_JUNE_SIGNATURE` \n",
      "    where\n",
      "        MOD(ABS(FARM_FINGERPRINT(\n",
      "            CONCAT(DATE,AIRLINE,ARR)\n",
      "        )) + DEP_T, 10000) >= 0 \n",
      "    and\n",
      "        MOD(ABS(FARM_FINGERPRINT(\n",
      "            CONCAT( DATE, AIRLINE, ARR)\n",
      "        )) + DEP_T, 10000) < 1 \n",
      "    \n",
      "Only 37 examples. Showing first three:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_T</th>\n",
       "      <th>ARR</th>\n",
       "      <th>AIRLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>40.77</td>\n",
       "      <td>-73.87</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>80.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2140</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.13</td>\n",
       "      <td>-76.49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1130</td>\n",
       "      <td>PHF</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>77.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>32.89</td>\n",
       "      <td>-80.04</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>84.1</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1505</td>\n",
       "      <td>CHS</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEP_LAT  DEP_LON  DEP_DELAY  MEAN_TEMP_DEP  MEAN_VIS_DEP  WND_SPD_DEP  \\\n",
       "0    33.63   -84.42        1.0           83.9          10.0          6.2   \n",
       "1    33.63   -84.42       -3.0           71.0           9.9          6.0   \n",
       "2    33.63   -84.42       -7.0           77.9           9.7          5.4   \n",
       "\n",
       "   ARR_LAT  ARR_LON  ARR_DELAY  MEAN_TEMP_ARR  MEAN_VIS_ARR  WND_SPD_ARR  \\\n",
       "0    40.77   -73.87       -2.0           80.7           8.9          5.7   \n",
       "1    37.13   -76.49        1.0           65.5           9.9          6.5   \n",
       "2    32.89   -80.04       -6.0           84.1           8.4          8.6   \n",
       "\n",
       "   DEP_DOW  DEP_T  ARR AIRLINE  \n",
       "0        1   2140  LGA      DL  \n",
       "1        1   1130  PHF      FL  \n",
       "2        1   1505  CHS      DL  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train.model_config import SIGNATURE_FLOAT_COLUMNS\n",
    "from train.model_config import SIGNATURE_INT_COLUMNS\n",
    "from train.model_config import SIGNATURE_STR_COLUMNS\n",
    "from signature_queries import sample_query\n",
    "\n",
    "SIGNATURE_COLUMNS = SIGNATURE_FLOAT_COLUMNS+SIGNATURE_INT_COLUMNS+SIGNATURE_STR_COLUMNS\n",
    "signature_query=sample_query(SIGNATURE_COLUMNS, total=10000)\n",
    "print(signature_query)\n",
    "sample = dlbq.Query(signature_query).execute().result().to_dataframe()\n",
    "print('Only {} examples. Showing first three:'.format(len(sample)))\n",
    "sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"WND_SPD_DEP\": 6.2, \"DEP_DELAY\": 1.0, \"ARR_LAT\": 40.77, \"WND_SPD_ARR\": 5.7, \"MEAN_VIS_DEP\": 10.0, \"DEP_T\": 2140, \"MEAN_TEMP_ARR\": 80.7, \"DEP_LON\": -84.42, \"DEP_DOW\": 1, \"MEAN_VIS_ARR\": 8.9, \"ARR\": \"LGA\", \"AIRLINE\": \"DL\", \"MEAN_TEMP_DEP\": 83.9, \"DEP_LAT\": 33.63, \"ARR_LON\": -73.87}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "samplefile='/tmp/test.json'\n",
    "a_record = sample.to_dict(orient='records')[0]\n",
    "a_record.pop('ARR_DELAY')\n",
    "with open(samplefile, 'w') as f:\n",
    "    f.write(json.dumps(a_record))\n",
    "!cat $samplefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Local prediction with gcloud ml-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://going-tfx/samples/model/export/exporter/1542638758/']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_versions = !gsutil ls $model_dir/export/exporter\n",
    "all_versions[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your version from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://going-tfx/samples/model/export/exporter/1542638758/'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_model=!gsutil ls gs://going-tfx/$DATASET/model/export/exporter | sort | tail -1\n",
    "latest_model = latest_model[0]\n",
    "latest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m 2018-11-19 14:47:04.171592: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\n",
      "OUTPUT\n",
      "[18.346132278442383]\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local predict \\\n",
    "  --model-dir=$latest_model \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"WND_SPD_DEP\": 4.8, \"DEP_DELAY\": 102.0, \"ARR_LAT\": 36.89, \"WND_SPD_ARR\": 7.4, \"MEAN_VIS_DEP\": 8.7, \"DEP_T\": 1728, \"MEAN_TEMP_ARR\": 77.6, \"DEP_LON\": -84.42, \"DEP_DOW\": 4, \"MEAN_VIS_ARR\": 8.9, \"ARR\": \"ORF\", \"AIRLINE\": \"EV\", \"MEAN_TEMP_DEP\": 78.4, \"DEP_LAT\": 33.63, \"ARR_LON\": -76.2}\n",
      "{\"WND_SPD_DEP\": 8.4, \"DEP_DELAY\": -2.0, \"ARR_LAT\": 26.68, \"WND_SPD_ARR\": 4.5, \"MEAN_VIS_DEP\": 8.5, \"DEP_T\": 1605, \"MEAN_TEMP_ARR\": 83.1, \"DEP_LON\": -84.42, \"DEP_DOW\": 4, \"MEAN_VIS_ARR\": 9.3, \"ARR\": \"PBI\", \"AIRLINE\": \"DL\", \"MEAN_TEMP_DEP\": 74.1, \"DEP_LAT\": 33.63, \"ARR_LON\": -80.09}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "samplefile='/tmp/test.json'\n",
    "records = sample.to_dict(orient='records')[:20]\n",
    "with open(samplefile, 'w') as f:\n",
    "    for record in records:\n",
    "        record.pop('ARR_DELAY')\n",
    "        f.write(json.dumps(record))\n",
    "        f.write('\\n')\n",
    "!cat $samplefile | tail -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Predicting from within python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=!gsutil ls gs://going-tfx/samples/model/export/exporter | sort | tail -1\n",
    "\n",
    "latest_model=latest_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://going-tfx/samples/model/export/exporter/1542640488/variables/variables\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.contrib.predictor.from_saved_model(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa718530bd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "compare=sample.copy()\n",
    "predicted = estimator(sample.to_dict(orient='list'))\n",
    "compare['PREDICTED'] = predicted['output']\n",
    "compare.plot.scatter(x='ARR_DELAY', y='PREDICTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### run the training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############################################################################################################\n",
      "using directory gs://going-tfx/samples/model to store the model. Remove the directory if you want to start from scratch\n",
      "###############################################################################################################\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'gs://going-tfx/samples/model'\n",
    "print(\"\\n###############################################################################################################\")\n",
    "print(\"using directory {} to store the model. Remove the directory if you want to start from scratch\".format(model_dir))\n",
    "print(\"###############################################################################################################\")\n",
    "_ = !gsutil rm -rf $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5c89ecbb50>, '_model_dir': 'gs://going-tfx/samples/model/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/wgiersche/py2/local/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-11-19 15:11:08.254826: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into gs://going-tfx/samples/model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1797.329, step = 1\n",
      "INFO:tensorflow:global_step/sec: 91.8306\n",
      "INFO:tensorflow:loss = 1698.4597, step = 101 (1.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.938\n",
      "INFO:tensorflow:loss = 1335.083, step = 201 (0.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.098\n",
      "INFO:tensorflow:loss = 2500.0054, step = 301 (0.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.768\n",
      "INFO:tensorflow:loss = 1437.2058, step = 401 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.2716\n",
      "INFO:tensorflow:loss = 1759.7329, step = 501 (1.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.695\n",
      "INFO:tensorflow:loss = 1814.4194, step = 601 (0.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.468\n",
      "INFO:tensorflow:loss = 1535.3213, step = 701 (0.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.512\n",
      "INFO:tensorflow:loss = 1285.5912, step = 801 (0.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.842\n",
      "INFO:tensorflow:loss = 1240.4218, step = 901 (0.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.6003\n",
      "INFO:tensorflow:loss = 1704.613, step = 1001 (1.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.5119\n",
      "INFO:tensorflow:loss = 1392.0182, step = 1101 (1.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.893\n",
      "INFO:tensorflow:loss = 1400.5773, step = 1201 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.436\n",
      "INFO:tensorflow:loss = 1683.9666, step = 1301 (0.761 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.664\n",
      "INFO:tensorflow:loss = 1455.4622, step = 1401 (0.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.953\n",
      "INFO:tensorflow:loss = 1733.8962, step = 1501 (0.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.323\n",
      "INFO:tensorflow:loss = 1395.7272, step = 1601 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.408\n",
      "INFO:tensorflow:loss = 1163.8187, step = 1701 (0.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.6691\n",
      "INFO:tensorflow:loss = 1740.155, step = 1801 (2.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.128\n",
      "INFO:tensorflow:loss = 1465.034, step = 1901 (0.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.917\n",
      "INFO:tensorflow:loss = 1351.5139, step = 2001 (0.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.499\n",
      "INFO:tensorflow:loss = 1170.6096, step = 2101 (0.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.202\n",
      "INFO:tensorflow:loss = 1629.4321, step = 2201 (0.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.705\n",
      "INFO:tensorflow:loss = 2452.4846, step = 2301 (0.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.9379\n",
      "INFO:tensorflow:loss = 1484.997, step = 2401 (1.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.108\n",
      "INFO:tensorflow:loss = 1633.2749, step = 2501 (0.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.675\n",
      "INFO:tensorflow:loss = 2041.1827, step = 2601 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.232\n",
      "INFO:tensorflow:loss = 2023.8499, step = 2701 (0.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.742\n",
      "INFO:tensorflow:loss = 1093.7949, step = 2801 (0.754 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.5986\n",
      "INFO:tensorflow:loss = 1489.0056, step = 2901 (1.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.6843\n",
      "INFO:tensorflow:loss = 1613.1802, step = 3001 (1.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.313\n",
      "INFO:tensorflow:loss = 1543.294, step = 3101 (0.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.201\n",
      "INFO:tensorflow:loss = 1531.2009, step = 3201 (0.751 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.752\n",
      "INFO:tensorflow:loss = 1091.9398, step = 3301 (0.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.819\n",
      "INFO:tensorflow:loss = 1485.79, step = 3401 (0.795 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.0989\n",
      "INFO:tensorflow:loss = 1320.9742, step = 3501 (1.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.3671\n",
      "INFO:tensorflow:loss = 1276.1835, step = 3601 (1.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.8035\n",
      "INFO:tensorflow:loss = 1358.4983, step = 3701 (1.033 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.918\n",
      "INFO:tensorflow:loss = 1842.9196, step = 3801 (0.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.996\n",
      "INFO:tensorflow:loss = 1462.0603, step = 3901 (0.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.354\n",
      "INFO:tensorflow:loss = 1246.9921, step = 4001 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.435\n",
      "INFO:tensorflow:loss = 1520.0326, step = 4101 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.664\n",
      "INFO:tensorflow:loss = 1859.4729, step = 4201 (0.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.6848\n",
      "INFO:tensorflow:loss = 1550.2036, step = 4301 (1.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.646\n",
      "INFO:tensorflow:loss = 1497.7126, step = 4401 (0.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.039\n",
      "INFO:tensorflow:loss = 1445.2372, step = 4501 (0.800 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.346\n",
      "INFO:tensorflow:loss = 1354.9722, step = 4601 (0.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.996\n",
      "INFO:tensorflow:loss = 1166.5947, step = 4701 (0.763 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.686\n",
      "INFO:tensorflow:loss = 1545.527, step = 4801 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.307\n",
      "INFO:tensorflow:loss = 1237.1016, step = 4901 (1.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.4927\n",
      "INFO:tensorflow:loss = 1743.3713, step = 5001 (1.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.87\n",
      "INFO:tensorflow:loss = 1834.8318, step = 5101 (0.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.26\n",
      "INFO:tensorflow:loss = 1344.2625, step = 5201 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.146\n",
      "INFO:tensorflow:loss = 1028.5591, step = 5301 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.192\n",
      "INFO:tensorflow:loss = 1121.1895, step = 5401 (0.811 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.4664\n",
      "INFO:tensorflow:loss = 1163.916, step = 5501 (1.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.2422\n",
      "INFO:tensorflow:loss = 1346.6992, step = 5601 (1.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.5561\n",
      "INFO:tensorflow:loss = 1415.1057, step = 5701 (1.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.133\n",
      "INFO:tensorflow:loss = 1109.5938, step = 5801 (0.792 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.802\n",
      "INFO:tensorflow:loss = 1414.3335, step = 5901 (0.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.142\n",
      "INFO:tensorflow:loss = 1613.2821, step = 6001 (0.792 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.9707\n",
      "INFO:tensorflow:loss = 1207.6475, step = 6101 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9357\n",
      "INFO:tensorflow:loss = 1200.197, step = 6201 (1.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.537\n",
      "INFO:tensorflow:loss = 1379.5833, step = 6301 (0.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.953\n",
      "INFO:tensorflow:loss = 1465.2676, step = 6401 (0.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.076\n",
      "INFO:tensorflow:loss = 1297.823, step = 6501 (0.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.964\n",
      "INFO:tensorflow:loss = 1256.0359, step = 6601 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.82\n",
      "INFO:tensorflow:loss = 1022.6012, step = 6701 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.7021\n",
      "INFO:tensorflow:loss = 1147.7583, step = 6801 (1.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.9044\n",
      "INFO:tensorflow:loss = 1227.2533, step = 6901 (1.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.91\n",
      "INFO:tensorflow:loss = 1103.617, step = 7001 (0.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.594\n",
      "INFO:tensorflow:loss = 2017.9578, step = 7101 (0.754 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.914\n",
      "INFO:tensorflow:loss = 1155.3894, step = 7201 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.849\n",
      "INFO:tensorflow:loss = 994.6061, step = 7301 (0.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.2772\n",
      "INFO:tensorflow:loss = 1123.4017, step = 7401 (1.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2026\n",
      "INFO:tensorflow:loss = 1673.9861, step = 7501 (1.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.9728\n",
      "INFO:tensorflow:loss = 1340.2507, step = 7601 (1.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.095\n",
      "INFO:tensorflow:loss = 1415.0574, step = 7701 (0.768 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.206\n",
      "INFO:tensorflow:loss = 851.12134, step = 7801 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.563\n",
      "INFO:tensorflow:loss = 1365.5908, step = 7901 (0.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.703\n",
      "INFO:tensorflow:loss = 1482.3168, step = 8001 (0.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.596\n",
      "INFO:tensorflow:loss = 1650.0382, step = 8101 (0.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.7904\n",
      "INFO:tensorflow:loss = 1314.7786, step = 8201 (1.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.043\n",
      "INFO:tensorflow:loss = 1153.7021, step = 8301 (0.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.383\n",
      "INFO:tensorflow:loss = 1052.8438, step = 8401 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.785\n",
      "INFO:tensorflow:loss = 1792.6322, step = 8501 (0.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.172\n",
      "INFO:tensorflow:loss = 1546.6943, step = 8601 (0.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.8867\n",
      "INFO:tensorflow:loss = 1100.9954, step = 8701 (1.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9504\n",
      "INFO:tensorflow:loss = 2184.802, step = 8801 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.864\n",
      "INFO:tensorflow:loss = 1542.3096, step = 8901 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.695\n",
      "INFO:tensorflow:loss = 1086.364, step = 9001 (0.796 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.451\n",
      "INFO:tensorflow:loss = 1206.9072, step = 9101 (0.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.302\n",
      "INFO:tensorflow:loss = 1413.1184, step = 9201 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.85\n",
      "INFO:tensorflow:loss = 1426.0057, step = 9301 (0.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.997\n",
      "INFO:tensorflow:loss = 1405.1262, step = 9401 (1.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.9373\n",
      "INFO:tensorflow:loss = 1201.7405, step = 9501 (1.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.378\n",
      "INFO:tensorflow:loss = 1071.2881, step = 9601 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.604\n",
      "INFO:tensorflow:loss = 1556.2512, step = 9701 (0.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.692\n",
      "INFO:tensorflow:loss = 1183.3414, step = 9801 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.604\n",
      "INFO:tensorflow:loss = 1529.8595, step = 9901 (0.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.5377\n",
      "INFO:tensorflow:loss = 1065.3707, step = 10001 (1.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.6982\n",
      "INFO:tensorflow:loss = 1097.558, step = 10101 (1.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.463\n",
      "INFO:tensorflow:loss = 1128.4929, step = 10201 (0.830 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.971\n",
      "INFO:tensorflow:loss = 990.5091, step = 10301 (0.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.752\n",
      "INFO:tensorflow:loss = 1387.189, step = 10401 (0.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.496\n",
      "INFO:tensorflow:loss = 1312.9357, step = 10501 (0.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.359\n",
      "INFO:tensorflow:loss = 1217.7686, step = 10601 (0.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.8056\n",
      "INFO:tensorflow:loss = 971.83813, step = 10701 (1.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.6615\n",
      "INFO:tensorflow:loss = 1094.4714, step = 10801 (1.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.337\n",
      "INFO:tensorflow:loss = 1364.3088, step = 10901 (0.761 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.724\n",
      "INFO:tensorflow:loss = 1106.4032, step = 11001 (0.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.045\n",
      "INFO:tensorflow:loss = 1190.0236, step = 11101 (0.799 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.4644\n",
      "INFO:tensorflow:loss = 1251.4004, step = 11201 (1.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0544\n",
      "INFO:tensorflow:loss = 1029.6582, step = 11301 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.238\n",
      "INFO:tensorflow:loss = 1280.1628, step = 11401 (0.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.305\n",
      "INFO:tensorflow:loss = 814.5636, step = 11501 (1.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.2538\n",
      "INFO:tensorflow:loss = 1328.3353, step = 11601 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.506\n",
      "INFO:tensorflow:loss = 876.7733, step = 11701 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.169\n",
      "INFO:tensorflow:loss = 1311.7112, step = 11801 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.52\n",
      "INFO:tensorflow:loss = 888.5564, step = 11901 (0.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0041\n",
      "INFO:tensorflow:loss = 1481.5767, step = 12001 (1.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.901\n",
      "INFO:tensorflow:loss = 1181.2053, step = 12101 (0.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.212\n",
      "INFO:tensorflow:loss = 1607.6583, step = 12201 (0.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.007\n",
      "INFO:tensorflow:loss = 1443.9266, step = 12301 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.34\n",
      "INFO:tensorflow:loss = 1370.6998, step = 12401 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.196\n",
      "INFO:tensorflow:loss = 1204.0732, step = 12501 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.557\n",
      "INFO:tensorflow:loss = 1163.0159, step = 12601 (0.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.2829\n",
      "INFO:tensorflow:loss = 1178.4187, step = 12701 (1.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.978\n",
      "INFO:tensorflow:loss = 1010.71405, step = 12801 (0.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.153\n",
      "INFO:tensorflow:loss = 1700.8667, step = 12901 (0.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.623\n",
      "INFO:tensorflow:loss = 1367.2902, step = 13001 (0.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4828\n",
      "INFO:tensorflow:loss = 1216.2295, step = 13101 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7419\n",
      "INFO:tensorflow:loss = 1367.7466, step = 13201 (1.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.8503\n",
      "INFO:tensorflow:loss = 1567.525, step = 13301 (1.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.2989\n",
      "INFO:tensorflow:loss = 1686.8113, step = 13401 (1.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.74\n",
      "INFO:tensorflow:loss = 1080.9307, step = 13501 (0.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.889\n",
      "INFO:tensorflow:loss = 1251.5735, step = 13601 (0.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4925\n",
      "INFO:tensorflow:loss = 1157.6903, step = 13701 (1.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.6948\n",
      "INFO:tensorflow:loss = 1090.3608, step = 13801 (1.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.54\n",
      "INFO:tensorflow:loss = 1334.9175, step = 13901 (0.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.306\n",
      "INFO:tensorflow:loss = 1766.0315, step = 14001 (0.792 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.24\n",
      "INFO:tensorflow:loss = 1599.0339, step = 14101 (0.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.131\n",
      "INFO:tensorflow:loss = 1204.5336, step = 14201 (0.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.3961\n",
      "INFO:tensorflow:loss = 1563.6697, step = 14301 (1.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.9713\n",
      "INFO:tensorflow:loss = 1597.3168, step = 14401 (1.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.367\n",
      "INFO:tensorflow:loss = 912.701, step = 14501 (0.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.179\n",
      "INFO:tensorflow:loss = 1138.8453, step = 14601 (0.825 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.208\n",
      "INFO:tensorflow:loss = 1123.5398, step = 14701 (0.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.203\n",
      "INFO:tensorflow:loss = 1521.6797, step = 14801 (0.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.255\n",
      "INFO:tensorflow:loss = 1548.438, step = 14901 (0.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2225\n",
      "INFO:tensorflow:loss = 1238.6245, step = 15001 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.787\n",
      "INFO:tensorflow:loss = 1082.6182, step = 15101 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.061\n",
      "INFO:tensorflow:loss = 1435.4918, step = 15201 (0.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.888\n",
      "INFO:tensorflow:loss = 1254.5635, step = 15301 (0.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.353\n",
      "INFO:tensorflow:loss = 1081.7877, step = 15401 (0.761 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.157\n",
      "INFO:tensorflow:loss = 1638.3234, step = 15501 (0.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.4263\n",
      "INFO:tensorflow:loss = 856.4377, step = 15601 (1.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8069\n",
      "INFO:tensorflow:loss = 1421.1238, step = 15701 (1.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.637\n",
      "INFO:tensorflow:loss = 1032.6931, step = 15801 (0.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.328\n",
      "INFO:tensorflow:loss = 1090.7744, step = 15901 (0.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.087\n",
      "INFO:tensorflow:loss = 1263.4744, step = 16001 (0.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.709\n",
      "INFO:tensorflow:loss = 1228.4575, step = 16101 (0.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.692\n",
      "INFO:tensorflow:loss = 1430.1748, step = 16201 (0.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8627\n",
      "INFO:tensorflow:loss = 843.8347, step = 16301 (1.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.7676\n",
      "INFO:tensorflow:loss = 891.28534, step = 16401 (1.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.222\n",
      "INFO:tensorflow:loss = 954.299, step = 16501 (0.792 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.045\n",
      "INFO:tensorflow:loss = 1055.7898, step = 16601 (0.751 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.087\n",
      "INFO:tensorflow:loss = 1241.0969, step = 16701 (0.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.495\n",
      "INFO:tensorflow:loss = 1267.4792, step = 16801 (0.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.648\n",
      "INFO:tensorflow:loss = 1407.9761, step = 16901 (0.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.3883\n",
      "INFO:tensorflow:loss = 1060.1472, step = 17001 (1.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.673\n",
      "INFO:tensorflow:loss = 1188.7002, step = 17101 (0.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.706\n",
      "INFO:tensorflow:loss = 1625.4446, step = 17201 (0.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.214\n",
      "INFO:tensorflow:loss = 1132.0334, step = 17301 (0.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.721\n",
      "INFO:tensorflow:loss = 1315.7854, step = 17401 (0.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.7682\n",
      "INFO:tensorflow:loss = 962.90295, step = 17501 (1.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6255\n",
      "INFO:tensorflow:loss = 1394.186, step = 17601 (1.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.829\n",
      "INFO:tensorflow:loss = 1268.8022, step = 17701 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.782\n",
      "INFO:tensorflow:loss = 806.0333, step = 17801 (0.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.56\n",
      "INFO:tensorflow:loss = 847.7743, step = 17901 (0.816 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.626\n",
      "INFO:tensorflow:loss = 1217.4733, step = 18001 (0.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.962\n",
      "INFO:tensorflow:loss = 1074.0607, step = 18101 (0.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4825\n",
      "INFO:tensorflow:loss = 821.2947, step = 18201 (1.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.3698\n",
      "INFO:tensorflow:loss = 1017.31067, step = 18301 (1.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.011\n",
      "INFO:tensorflow:loss = 1552.5125, step = 18401 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.457\n",
      "INFO:tensorflow:loss = 1815.0859, step = 18501 (0.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.949\n",
      "INFO:tensorflow:loss = 1080.4454, step = 18601 (0.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.345\n",
      "INFO:tensorflow:loss = 1264.2563, step = 18701 (0.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0695\n",
      "INFO:tensorflow:loss = 1958.322, step = 18801 (1.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.6055\n",
      "INFO:tensorflow:loss = 1116.6221, step = 18901 (1.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.434\n",
      "INFO:tensorflow:loss = 1831.7108, step = 19001 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.134\n",
      "INFO:tensorflow:loss = 1016.9984, step = 19101 (0.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.785\n",
      "INFO:tensorflow:loss = 784.78723, step = 19201 (0.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.059\n",
      "INFO:tensorflow:loss = 1526.4116, step = 19301 (0.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.416\n",
      "INFO:tensorflow:loss = 1289.9476, step = 19401 (0.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.0398\n",
      "INFO:tensorflow:loss = 907.7517, step = 19501 (1.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.762\n",
      "INFO:tensorflow:loss = 1070.5493, step = 19601 (0.783 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.571\n",
      "INFO:tensorflow:loss = 1151.5261, step = 19701 (0.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.024\n",
      "INFO:tensorflow:loss = 1251.2086, step = 19801 (0.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.879\n",
      "INFO:tensorflow:loss = 850.00696, step = 19901 (0.814 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into gs://going-tfx/samples/model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-19-15:14:38\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://going-tfx/samples/model/model.ckpt-20000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [51/512]\n",
      "INFO:tensorflow:Evaluation [102/512]\n",
      "INFO:tensorflow:Evaluation [153/512]\n",
      "INFO:tensorflow:Evaluation [204/512]\n",
      "INFO:tensorflow:Evaluation [255/512]\n",
      "INFO:tensorflow:Evaluation [306/512]\n",
      "INFO:tensorflow:Evaluation [357/512]\n",
      "INFO:tensorflow:Evaluation [408/512]\n",
      "INFO:tensorflow:Evaluation [459/512]\n",
      "INFO:tensorflow:Evaluation [510/512]\n",
      "INFO:tensorflow:Evaluation [512/512]\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-19-15:14:45\n",
      "INFO:tensorflow:Saving dict for global step 20000: global_step = 20000, loss = 1383.2699\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20000: gs://going-tfx/samples/model/model.ckpt-20000\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from gs://going-tfx/samples/model/model.ckpt-20000\n",
      "WARNING:tensorflow:From /home/wgiersche/py2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py:1044: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: gs://going-tfx/samples/model/export/exporter/temp-1542640488/assets\n",
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/model/export/exporter/temp-1542640488/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 1225.957.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# --distribute to use both GPUs with a resulting performance penalty - hence: Don't!\n",
    "\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
    "python -m train.task \\\n",
    "  --base_dir=gs://going-tfx/samples/ \\\n",
    "  --metadata_dir=metadata \\\n",
    "  --train_data_pattern=\"train_data/atl_june_tfr*\" \\\n",
    "  --eval_data_pattern=\"eval_data/atl_june_tfr*\"  \\\n",
    "  --model_dir=model \\\n",
    "  --max_train_steps=20000 \\\n",
    "  --learning_rate=1e-3 \\\n",
    "  --job-dir=/tmp \\\n",
    "  --prefetch_buffer_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
