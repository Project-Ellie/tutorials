{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./prep\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Signature data in Bigquery\n",
    "We collected the raw data that we use from various sources into a single denormalized table holding the data in so-called signature format. That table's schema is meant to reflect the structure of the data/requests that we expect to be served at prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>DEP_T</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>...</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "      <th>ARR_T</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>ARR_W</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-06-01</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>1927</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>...</td>\n",
       "      <td>14.4</td>\n",
       "      <td>2113</td>\n",
       "      <td>26.0</td>\n",
       "      <td>RNO</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-119.76</td>\n",
       "      <td>RENO WBO</td>\n",
       "      <td>67.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-06-02</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>1927</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>2113</td>\n",
       "      <td>40.0</td>\n",
       "      <td>RNO</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-119.76</td>\n",
       "      <td>RENO WBO</td>\n",
       "      <td>62.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-06-03</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>1927</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2113</td>\n",
       "      <td>13.0</td>\n",
       "      <td>RNO</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-119.76</td>\n",
       "      <td>RENO WBO</td>\n",
       "      <td>58.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  YEAR  MONTH  DAY  DEP_DOW                   AIRLINE  DEP_T  \\\n",
       "0  2005-06-01  2005      6    1        4  Delta Air Lines Inc.: DL   1927   \n",
       "1  2005-06-02  2005      6    2        5  Delta Air Lines Inc.: DL   1927   \n",
       "2  2005-06-03  2005      6    3        6  Delta Air Lines Inc.: DL   1927   \n",
       "\n",
       "   DEP  DEP_LAT  DEP_LON     ...       WND_SPD_DEP ARR_T  ARR_DELAY  ARR  \\\n",
       "0  ATL    33.63   -84.42     ...              14.4  2113       26.0  RNO   \n",
       "1  ATL    33.63   -84.42     ...               9.7  2113       40.0  RNO   \n",
       "2  ATL    33.63   -84.42     ...               7.4  2113       13.0  RNO   \n",
       "\n",
       "   ARR_LAT  ARR_LON     ARR_W MEAN_TEMP_ARR  MEAN_VIS_ARR  WND_SPD_ARR  \n",
       "0    39.49  -119.76  RENO WBO          67.2          10.0          9.8  \n",
       "1    39.49  -119.76  RENO WBO          62.1          10.0          7.2  \n",
       "2    39.49  -119.76  RENO WBO          58.9          10.0          4.3  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery sample\n",
    "select * FROM `going-tfx.examples.ATL_JUNE_SIGNATURE` limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEP_DOW', 'DEP_T', 'DEP_LAT', 'DEP_LON', 'DEP_DELAY', 'MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'ARR_LAT', 'ARR_LON', 'ARR_DELAY', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR']\n"
     ]
    }
   ],
   "source": [
    "from train.model_config import SIGNATURE_COLUMNS\n",
    "print(SIGNATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_queries written to ./prep/sample_queries.py.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_queries(columns, fractions, rate=0.1):\n",
    "\n",
    "    def sample_query(columns, total, lower, upper):\n",
    "        col_string=\", \".join(columns)\n",
    "        return \"\"\"\n",
    "        SELECT\n",
    "            {0}\n",
    "        FROM \n",
    "            `going-tfx.examples.ATL_JUNE_SIGNATURE` \n",
    "        where\n",
    "            MOD(ABS(FARM_FINGERPRINT(\n",
    "                CONCAT(DATE,AIRLINE,ARR)\n",
    "            )) + DEP_T, {1}) >= {2} \n",
    "        and\n",
    "            MOD(ABS(FARM_FINGERPRINT(\n",
    "                CONCAT( DATE, AIRLINE, ARR)\n",
    "            )) + DEP_T, {1}) < {3} \n",
    "        \"\"\".format(col_string, total, lower, upper)\n",
    "    \n",
    "    start = 0\n",
    "    total = int(sum(fractions) / rate)\n",
    "    res = []\n",
    "    for f in fractions:\n",
    "        f_ = int(f) \n",
    "        q = sample_query(columns, total, start, start+f_)\n",
    "        start = start + f_\n",
    "        res.append(q)\n",
    "    return dict(zip(['train', 'eval', 'test'], res))\n",
    "write_py(sample_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### The pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre_process written to ./prep/pre_process.py.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_process(row):\n",
    "    import tensorflow_transform as tft\n",
    "    from tools import tf_haversine\n",
    "\n",
    "    def add_engineered(row):\n",
    "        dep_lat = row['DEP_LAT']\n",
    "        dep_lon = row['DEP_LON']\n",
    "        arr_lat = row['ARR_LAT']\n",
    "        arr_lon = row['ARR_LON']\n",
    "\n",
    "        row['DEP_HOD'] = row['DEP_T'] // 100\n",
    "        row.pop('DEP_T')  # no longer needed\n",
    "\n",
    "        row['DIFF_LAT'] = arr_lat - dep_lat\n",
    "        row['DIFF_LON'] = arr_lon - dep_lon\n",
    "        row['DISTANCE'] = tf_haversine(arr_lat, arr_lon, dep_lat, dep_lon)\n",
    "        return row\n",
    "\n",
    "    def scale_floats(row):\n",
    "        for c in ['MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR', 'DEP_DELAY',\n",
    "                 'DIFF_LAT', 'DIFF_LON', 'DISTANCE']:\n",
    "            row[c] = tft.scale_to_0_1(row[c])\n",
    "        return row\n",
    "    \n",
    "    row = row.copy()\n",
    "    row = add_engineered(row)\n",
    "    row = scale_floats(row)\n",
    "    return row\n",
    "write_py(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEP_DOW', 'DEP_HOD', 'DEP_LAT', 'DEP_LON', 'MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'DEP_DELAY', 'ARR_LAT', 'ARR_LON', 'ARR_DELAY', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR', 'DIFF_LAT', 'DIFF_LON', 'DISTANCE']\n"
     ]
    }
   ],
   "source": [
    "from train.model_config import TRAINING_COLUMNS\n",
    "print(TRAINING_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### The full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exec_pipeline_prod written to ./prep/exec_pipeline_prod.py.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exec_pipeline_prod (options, train_dir, eval_dir, test_dir, \n",
    "                        metadata_dir, tmp_dir,\n",
    "                        fractions, sample_rate, prefix,\n",
    "                        runner='DirectRunner'):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_transform as tft\n",
    "    import tensorflow_transform.beam.impl as beam_impl\n",
    "    import apache_beam as beam\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "    \n",
    "    from train.model_config import (SIGNATURE_COLUMNS, TRAINING_COLUMNS,\n",
    "        SIGNATURE_METADATA)\n",
    "    from prep.pre_process import pre_process\n",
    "    from prep.sample_queries import sample_queries\n",
    "\n",
    "    \n",
    "    with beam.Pipeline(runner, options=options) as p:\n",
    "        with beam_impl.Context(temp_dir=tmp_dir):\n",
    "            \n",
    "            # Process training data and obtain transform_fn\n",
    "            #\n",
    "            queries = sample_queries(SIGNATURE_COLUMNS, fractions, sample_rate)\n",
    "\n",
    "            signature_data = (p | \"ReadFromBigQuery_train\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['train'], use_standard_sql=True)))\n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "            \n",
    "            tds, transform_fn = (signature_dataset | \"AnalyzeAndTransform\" \n",
    "                        >> beam_impl.AnalyzeAndTransformDataset(pre_process))\n",
    "            t_data, t_metadata = tds\n",
    "\n",
    "            train_prefix = os.path.join(train_dir, prefix)\n",
    "            encoder = tft.coders.ExampleProtoCoder(t_metadata.schema)\n",
    "\n",
    "            _ = (t_data\n",
    "                 | 'EncodeTFRecord_train' >> beam.Map(encoder.encode)\n",
    "                 | 'WriteTFRecord_train' >> beam.io.WriteToTFRecord(train_prefix))\n",
    "        \n",
    "        \n",
    "            #  Process evaluation data with the obtained transform_fn\n",
    "            #\n",
    "            signature_data = (p | \"ReadFromBigQuery_eval\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['eval'], use_standard_sql=True))) \n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "\n",
    "            t_dataset = ((signature_dataset, transform_fn) \n",
    "                         | \"TransformEval\" >> beam_impl.TransformDataset())\n",
    "            t_data, _ = t_dataset\n",
    "            eval_prefix = os.path.join(eval_dir, prefix)\n",
    "            _ = (t_data\n",
    "                 | 'EncodeTFRecord_eval' >> beam.Map(encoder.encode)\n",
    "                 | 'WriteTFRecord_eval' >> beam.io.WriteToTFRecord(eval_prefix))\n",
    "        \n",
    "            \n",
    "            #  Also process test data with the obtained transform_fn\n",
    "            #\n",
    "            signature_data = (p | \"ReadFromBigQuery_test\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['test'], use_standard_sql=True)))\n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "\n",
    "            t_dataset = ((signature_dataset, transform_fn) \n",
    "                         | \"TransformTest\" >> beam_impl.TransformDataset())\n",
    "            t_data, _ = t_dataset\n",
    "            test_prefix = os.path.join(test_dir, prefix)\n",
    "            _ = (t_data\n",
    "                 | 'EncodeTFRecord_test' >> beam.Map(encoder.encode)\n",
    "                 | 'WriteTFRecord_test' >> beam.io.WriteToTFRecord(test_prefix))\n",
    "        \n",
    "            \n",
    "            # save transforma function to disk for use at serving time\n",
    "            #\n",
    "            transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(metadata_dir)\n",
    "\n",
    "write_py(exec_pipeline_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run_job written to ./prep/run_job.py.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_job(args):\n",
    "    \n",
    "    import datetime\n",
    "    import apache_beam as beam\n",
    "    from prep.exec_pipeline_prod import exec_pipeline_prod\n",
    "    \n",
    "    job_name = 'tft-tutorial' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "    \n",
    "    options = {\n",
    "        'staging_location': args['stage_dir'],\n",
    "        'temp_location': args['tmp_dir'],\n",
    "        'job_name': job_name,\n",
    "        'project': args['project'],\n",
    "        'max_num_workers': int(args['max_workers']),\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'requirements_file': 'dataflow_requirements.txt'\n",
    "    }    \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "    fractions = [int(n) for n in args['fractions'].split(\",\")]\n",
    "\n",
    "    exec_pipeline_prod (opts, args['train_dir'], args['eval_dir'],args['test_dir'],\n",
    "                        args['metadata_dir'], args['tmp_dir'],\n",
    "                        fractions, float(args['sample_rate']), args['prefix'],\n",
    "                        runner=args['runner'])\n",
    "write_py(run_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(subproject):\n",
    "    import os\n",
    "    basedir = os.path.join('gs://going-tfx/', subproject)\n",
    "\n",
    "    for d in ['train_data/*', 'eval_data/*', 'test_data/*', 'tmp/*', 'model/*', 'metadata/*']:\n",
    "        target = os.path.join(basedir, d)\n",
    "        !echo gsutil -m rm -rf $target\n",
    "        _ = !gsutil -m rm -rf $target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -m rm -rf gs://going-tfx/sandbox/train_data/*\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/eval_data/*\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/test_data/*\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/tmp/*\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/model/*\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/metadata/*\n",
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "cleanup('sandbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/sandbox/tmp/tftransform_tmp/04dc720d643541f5a1370289a1d8c6b9/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/sandbox/tmp/tftransform_tmp/749ec2301cfe4692a80f7e54a7a0c6b7/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgiersche/py2/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:360: DeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_23fece76b1ea4e8c828d19df0f871fd3 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_4559389c02a44fb38969a2fbe0c8ea99 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_95d29d3c50df4531a494f3d70535aa4d does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/sandbox/tmp/tftransform_tmp/e3b2308c4b65417c9693d3802da79af7/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/sandbox/tmp/tftransform_tmp/e3b2308c4b65417c9693d3802da79af7/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    }
   ],
   "source": [
    "from prep.prep_tools import join_paths\n",
    "\n",
    "args={}\n",
    "args['base_dir'] = \"gs://going-tfx/sandbox\"\n",
    "args['train_dir'] = 'train_data'\n",
    "args['eval_dir'] = 'eval_data'\n",
    "args['test_dir'] = 'test_data'\n",
    "args['metadata_dir'] = 'metadata'\n",
    "args['stage_dir'] = 'staging'\n",
    "args['tmp_dir'] = 'tmp'\n",
    "args['project'] = 'going-tfx'\n",
    "args['prefix'] = 'atl_june'\n",
    "args['fractions'] = '90,5,5'\n",
    "args['sample_rate'] = 0.1\n",
    "args['max_workers'] = 24\n",
    "args['runner'] = 'DirectRunner'\n",
    "\n",
    "\n",
    "run_job(join_paths(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -m rm -rf gs://going-tfx/sandbox/train_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/eval_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/test_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/tmp/*\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/model/*\n",
      "gsutil -m rm -rf gs://going-tfx/sandbox/metadata/*\n"
     ]
    }
   ],
   "source": [
    "cleanup('sandbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgiersche/py2/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: ImportWarning: Not importing directory '/home/wgiersche/py2/local/lib/python2.7/site-packages/scipy/spatial/qhull': missing __init__.py\n",
      "  from .qhull import *\n",
      "/home/wgiersche/py2/local/lib/python2.7/site-packages/scipy/optimize/_minimize.py:37: ImportWarning: Not importing directory '/home/wgiersche/py2/local/lib/python2.7/site-packages/scipy/optimize/lbfgsb': missing __init__.py\n",
      "  from .lbfgsb import _minimize_lbfgsb\n",
      "2018-11-17 11:04:52.976471: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "/home/wgiersche/py2/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:360: DeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_3915e2916e8646f3bc01efb4dd0288c3 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_2e0f351913304399b9b5d3a00bbc0a1b does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_ec0d49fa4ffa40b595df1ee6f685b2da does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
    "python -m prep.task \\\n",
    "    --project=going-tfx \\\n",
    "    --base_dir=gs://going-tfx/sandbox/ \\\n",
    "    --sample_rate=0.1 \\\n",
    "    --prefix=atl_june"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
