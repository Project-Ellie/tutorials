{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a TFR Dataset with A-values rather than Q-values. ```TournamentData.ipynb``` should provide either another \"feature\" or another TFR file set for A-values. - DONE\n",
    "\n",
    "Then RunQVT.ipynb should be used to run the A-values. - DONE\n",
    "\n",
    "It should be possible to choose the number of games or records in RunQVT. Well, it is, I guess - through the options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The A-values are derived by subtracting the default value, which can be found beyond the border on a 22x22 board from the q value. - DONE\n",
    "\n",
    "This should then be divided by 100.0 and the learned A-value should eventually be multiplied by 100, if ever to be re-used in order to recreate the Q-Function. - DONE\n",
    "\n",
    "The mask is used to not have the beyond-the-border fields influence the loss function. - DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HeuristicGomokuPolicy's defense_options are buggy. - DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imitation learning\n",
    "\n",
    "We'll have a $\\pi$-$A$ network consisting of a common residual network with\n",
    "    - 5-8 layers for line recognition\n",
    "    - multiple residual blocks\n",
    "    - residual helps convergence and also reflects the stone-meets-line paradigm\n",
    "    - network design must allow for an stone-to-field influence across the entire board\n",
    "        - because the remotest stone could have a role in a threat sequence\n",
    "\n",
    "Trajectory production is super-slow. Need mass production for imitation training, because the heuristic threat search is slow.\n",
    "\n",
    "During imitation training, a single combined loss function should help $\\pi$ and $A$ network to initialize to efficient networks.\n",
    "\n",
    "Imitation learning means $\\pi$ network learns to predict the moves of the heuristic policy, while the $A$ network learns the advantage from the QFunction that's based on the same heuristics. \n",
    "\n",
    "We need the self-play recordings to generate the training data for imitation learning.\n",
    "\n",
    "Least-significant-move baseline LSMB: May take any other. Has the advantage of creating zeros for all but the relevant fields during the imitation learning phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deep RL\n",
    "There's only one type of reward: Win or loss. The reward is going to be discounted continuously back to the beginning of the trajectory and provided for $A$-learning.\n",
    "\n",
    "It's not clear how the different training phases ($\\pi$ and $A$) might influence each other. The network might get pushed forth and back.\n",
    "\n",
    "Maybe it's better to initialize a single network and RL-train $\\pi$ and $A$ independently thereafter.\n",
    "\n",
    "The weights (the critiques) are given by $[r_e - \\tilde{A}(s_t)]$, where $r_e$ is episode's final reward and $\\tilde{A}$ is the current advantage estimate for the least significant move after $s_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
