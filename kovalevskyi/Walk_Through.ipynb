{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "import tensorflow_transform.beam.impl as beam_impl\n",
    "\n",
    "from utilities.transforms import MapAndFilterErrors\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD=os.getcwd()\n",
    "BUCKET='going-tfx'\n",
    "PROJECT='going-tfx'\n",
    "EXTRA_PACKAGE='./utilities'\n",
    "DATA_DIR=\"gs://{}/flight_data\".format(BUCKET)\n",
    "OUTPUT_DIR=\"gs://{}/output\".format(BUCKET)\n",
    "TMP_DIR=\"gs://{}/tmp\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_COLUMNS = ['FL_DATE', 'FL_YEAR', 'FL_MONTH', 'FL_DOM', 'FL_DOW', 'UNIQUE_CARRIER', 'FL_NUM',\n",
    "       'ORIGIN_AIRPORT_SEQ_ID', 'DEST_AIRPORT_SEQ_ID', 'ORIGIN', 'DEST',\n",
    "       'CRS_DEP_TIME', 'DEP_DELAY', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON',\n",
    "       'TAXI_IN', 'CRS_ARR_TIME', 'ARR_DELAY', 'CANCELLED',\n",
    "       'CANCELLATION_CODE', 'DIVERTED', 'DISTANCE']\n",
    "DEFAULTS = [[\"-\"], [], [], [], [], [\"-\"], [\"-\"], [\"-\"], [\"-\"], [\"-\"], [\"-\"], [], [], [], [], [], [], [], [], [], ['NONE'], [], []]\n",
    "\n",
    "ORDERED_COLUMNS = ['FL_YEAR', 'FL_MONTH', 'FL_DOW', 'UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'CRS_DEP_TIME', 'DEP_DELAY', 'CRS_ARR_TIME', 'ARR_DELAY']\n",
    "\n",
    "SELECT = list(np.sort([ALL_COLUMNS.index(c) for c in ORDERED_COLUMNS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Get an impression of what's in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlanta_june = tf.data.TextLineDataset(os.path.join(DATA_DIR, 'atl_june.csv'))\n",
    "def decode_csv(row):\n",
    "    cols = tf.decode_csv(row, select_cols=SELECT, record_defaults=[DEFAULTS[i] for i in SELECT])\n",
    "    features = dict(zip([ALL_COLUMNS[i] for i in SELECT], cols))\n",
    "    return features\n",
    "\n",
    "inp = atlanta_june.skip(1).map(decode_csv).batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEST</th>\n",
       "      <th>FL_DOW</th>\n",
       "      <th>FL_MONTH</th>\n",
       "      <th>FL_YEAR</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>147.0</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>LFT</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ARR_DELAY  CRS_ARR_TIME  CRS_DEP_TIME  DEP_DELAY DEST  FL_DOW  FL_MONTH  \\\n",
       "0       78.0        1705.0        1621.0       55.0  LFT     4.0       6.0   \n",
       "1      -12.0        2056.0        2010.0       -2.0  LFT     4.0       6.0   \n",
       "2       66.0        1344.0        1305.0       63.0  LFT     4.0       6.0   \n",
       "3       -3.0        1051.0        1006.0       -3.0  LFT     4.0       6.0   \n",
       "4       19.0        1344.0        1305.0       -6.0  LFT     5.0       6.0   \n",
       "5      147.0        1051.0        1006.0      139.0  LFT     5.0       6.0   \n",
       "6      -10.0        2056.0        2010.0       -5.0  LFT     5.0       6.0   \n",
       "7       -6.0        1705.0        1621.0        0.0  LFT     5.0       6.0   \n",
       "8       27.0        1344.0        1305.0       21.0  LFT     6.0       6.0   \n",
       "9       -5.0        2056.0        2010.0       -4.0  LFT     6.0       6.0   \n",
       "\n",
       "   FL_YEAR ORIGIN UNIQUE_CARRIER  \n",
       "0   2016.0    ATL             EV  \n",
       "1   2016.0    ATL             DL  \n",
       "2   2016.0    ATL             EV  \n",
       "3   2016.0    ATL             EV  \n",
       "4   2016.0    ATL             EV  \n",
       "5   2016.0    ATL             EV  \n",
       "6   2016.0    ATL             DL  \n",
       "7   2016.0    ATL             EV  \n",
       "8   2016.0    ATL             EV  \n",
       "9   2016.0    ATL             DL  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_raw = inp.make_one_shot_iterator().get_next()\n",
    "with tf.Session() as sess:\n",
    "    b = sess.run(input_raw)\n",
    "pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Metadata and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_schema = {\n",
    "    colname : dataset_schema.ColumnSchema(\n",
    "        tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "               for colname in ['ORIGIN','UNIQUE_CARRIER','DEST']\n",
    "}\n",
    "raw_data_schema.update({\n",
    "    colname : dataset_schema.ColumnSchema(\n",
    "        tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "               for colname in ['DEP_DELAY','ARR_DELAY']\n",
    "})\n",
    "raw_data_schema.update({\n",
    "    colname : dataset_schema.ColumnSchema(\n",
    "        tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "               for colname in ['FL_YEAR','FL_MONTH','FL_DOW','CRS_DEP_TIME','CRS_ARR_TIME']\n",
    "})\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating meta data is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec({\n",
    "        'ORIGIN': tf.FixedLenFeature([], tf.string),\n",
    "        'FL_YEAR': tf.FixedLenFeature([], tf.int64),\n",
    "        'FL_MONTH': tf.FixedLenFeature([], tf.int64),\n",
    "        'FL_DOW': tf.FixedLenFeature([], tf.int64),\n",
    "        'UNIQUE_CARRIER': tf.FixedLenFeature([], tf.string),\n",
    "        'DEST': tf.FixedLenFeature([], tf.string),\n",
    "        'CRS_DEP_TIME': tf.FixedLenFeature([], tf.int64),\n",
    "        'CRS_ARR_TIME': tf.FixedLenFeature([], tf.int64),\n",
    "        'DEP_DELAY': tf.FixedLenFeature([], tf.float32),\n",
    "        'ARR_DELAY': tf.FixedLenFeature([], tf.float32)\n",
    "    }))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Preprocessing Pipeline \n",
    "Here, we scale ```DEP_DELAY``` to the range $[0,1]$ and select only the interesting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    outputs = inputs.copy()\n",
    "    outputs = {k: outputs[k] for k in ORDERED_COLUMNS}             # Projection: Select useful columns \n",
    "\n",
    "    outputs['DEP_DELAY'] = tft.scale_to_0_1(outputs['DEP_DELAY'])  # Scale \n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = os.path.join(DATA_DIR, 'atl_june.csv') # 403'358 records \n",
    "csv_decode = tft.coders.CsvCoder(ALL_COLUMNS, raw_data_metadata.schema).decode\n",
    "csv_encode = tft.coders.CsvCoder(ORDERED_COLUMNS, raw_data_metadata.schema).encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to use a smaller test file\n",
    "training_file = 'atl_june_11.csv' # 10 records only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check on for the error filter with a very small subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('atl_june_11.csv') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016,6,4,EV,ATL,LFT,1621,55.0,1705,78.0',\n",
       " '2016,6,4,DL,ATL,LFT,2010,-2.0,2056,-12.0',\n",
       " '2016,6,4,EV,ATL,LFT,1305,63.0,1344,66.0',\n",
       " '2016,6,4,EV,ATL,LFT,1006,-3.0,1051,-3.0',\n",
       " '2016,6,5,EV,ATL,LFT,1305,-6.0,1344,19.0',\n",
       " '2016,6,5,EV,ATL,LFT,1006,139.0,1051,147.0',\n",
       " '2016,6,5,DL,ATL,LFT,2010,-5.0,2056,-10.0',\n",
       " '2016,6,5,EV,ATL,LFT,1621,0.0,1705,-6.0',\n",
       " '2016,6,6,EV,ATL,LFT,1305,21.0,1344,27.0',\n",
       " '2016,6,6,DL,ATL,LFT,2010,-4.0,2056,-5.0']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(content \n",
    " | MapAndFilterErrors(csv_decode) \n",
    " | beam.Map(csv_encode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up any previously processed records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://going-tfx/output/atl_june_transformed-00000-of-00001...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!gsutil rm -f $OUTPUT_DIR/atl_june_transformed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Pipeline\n",
    "Runtime options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectRunner\n",
      "{'machine_type': None, 'runner': None, 'labels': None, 'save_main_session': False, 'streaming': False, 'experiments': None, 'requirements_cache': None, 'harness_docker_image': None, 'max_num_workers': 24, 'template_location': None, 'requirements_file': 'requirements.txt', 'sdk_location': 'default', 'network': None, 'dry_run': False, 'profile_location': None, 'service_account_email': None, 'profile_cpu': False, 'profile_memory': False, 'direct_runner_use_stacked_bundle': True, 'type_check_strictness': 'DEFAULT_TO_ANY', 'job_name': 'tft-tutorial-181026-212541', 'use_public_ips': None, 'num_workers': None, 'hdfs_host': None, 'disk_size_gb': None, 'runtime_type_check': False, 'on_success_matcher': None, 'temp_location': 'gs://going-tfx/tmp/tft-tutorial-181026-212541', 'setup_file': '/home/jupyter/workspace/home-in-time/setup.py', 'disk_type': None, 'dataflow_endpoint': 'https://dataflow.googleapis.com', 'worker_harness_container_image': None, 'hdfs_port': None, 'autoscaling_algorithm': None, 'zone': None, 'hdfs_user': None, 'dataflow_job_file': None, 'region': 'us-central1', 'staging_location': 'gs://going-tfx/tmp/tft-tutorial-181026-212541/staging', 'wait_until_finish_duration': None, 'pipeline_type_check': True, 'project': 'going-tfx', 'extra_packages': None, 'subnetwork': None, 'job_endpoint': None, 'beam_plugins': None, 'no_auth': False}\n"
     ]
    }
   ],
   "source": [
    "test_mode = True\n",
    "\n",
    "job_name = 'tft-tutorial-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "tmp_dir = os.path.join(TMP_DIR, job_name)\n",
    "options = {\n",
    "    'staging_location': os.path.join(tmp_dir, 'staging'),\n",
    "    'temp_location': tmp_dir,\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'max_num_workers': 24,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'requirements_file': 'requirements.txt',\n",
    "    'setup_file': os.path.join(PWD, 'setup.py')\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "if test_mode:\n",
    "    RUNNER = 'DirectRunner'\n",
    "else:\n",
    "    RUNNER = 'DataflowRunner'\n",
    "\n",
    "print(RUNNER)\n",
    "print(opts.get_all_options())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Construct and run the pipeline\n",
    "Be aware that this can take quite a while. You can watch your pipeline run on the dataflow console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline(RUNNER, options=opts) as pipeline:\n",
    "    with beam_impl.Context(temp_dir=tmp_dir):\n",
    "        \n",
    "        # Decode the raw data from CSV file and filter outliers\n",
    "        raw_data = (\n",
    "            pipeline \n",
    "            | 'ReadData' >> beam.io.ReadFromText(training_file, skip_header_lines=1)\n",
    "            | 'Decode' >> MapAndFilterErrors(csv_decode)\n",
    "            | 'Filter_outliers' >> beam.Filter(lambda r: r['DEP_DELAY'] < 120.0)\n",
    "        )\n",
    "        \n",
    "        # Analyse and transform - handle meta_data\n",
    "        raw_dataset = (raw_data, raw_data_metadata)\n",
    "        t_dataset, t_fn = (raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "        t_data, t_metadata = t_dataset\n",
    " \n",
    "        # Encode back to CSV file(s)\n",
    "        res = (t_data \n",
    "               | beam.Map(csv_encode)\n",
    "               | beam.io.WriteToText(file_path_prefix=os.path.join(OUTPUT_DIR, \"atl_june_transformed\")))\n",
    "            \n",
    "result = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If you want to wait for the result, this would be the way to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The resulting CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       403  2018-10-26T21:26:00Z  gs://going-tfx/output/atl_june_transformed-00000-of-00001\n",
      "TOTAL: 1 objects, 403 bytes (403 B)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l $OUTPUT_DIR/atl_june_transformed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat $OUTPUT_DIR/atl_june_transformed-00000-of-00001 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016,6,4,EV,ATL,LFT,1621,0.884058,1705,78.0\n",
      "2016,6,4,DL,ATL,LFT,2010,0.057971016,2056,-12.0\n",
      "2016,6,4,EV,ATL,LFT,1305,1.0,1344,66.0\n",
      "2016,6,4,EV,ATL,LFT,1006,0.04347826,1051,-3.0\n",
      "2016,6,5,EV,ATL,LFT,1305,0.0,1344,19.0\n",
      "2016,6,5,DL,ATL,LFT,2010,0.014492754,2056,-10.0\n",
      "2016,6,5,EV,ATL,LFT,1621,0.08695652,1705,-6.0\n",
      "2016,6,6,EV,ATL,LFT,1305,0.39130434,1344,27.0\n",
      "2016,6,6,DL,ATL,LFT,2010,0.028985508,2056,-5.0\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat $OUTPUT_DIR/atl_june_transformed-00000-of-00001 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PCollection[WriteToText/Write/WriteImpl/FinalizeWrite.None] at 0x7ffa14205690>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
