{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./train\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ```tf.data``` input functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These input functions read from any number of files containing pre-processed data\n",
    "In our parlour, this data is at the **training stage**. It's been fetched from a BigQuery table and pre-processed. It's not what'll come in at prediction time!\n",
    "\n",
    "That pre-processing function has been stored in a metadata directory that is available to us to treat data at prediction time (**signature stage**) exactly the same way that our training data has been treated.\n",
    "\n",
    "\n",
    "\n",
    "```make_XXX_input_fn()``` returns an input_function. This function will be passed to the estimator, such that the estimator can call it in its own session/graph context to create a particularly useful input tensor. That input tensor will return the next batch of input records whenever it is evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_input_fns written to ./train/make_input_fns.py.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_input_fns():\n",
    "    from train.make_csv_input_fn import make_csv_input_fn\n",
    "    from train.make_tfr_input_fn import make_tfr_input_fn\n",
    "    \n",
    "    return {\n",
    "        'csv': make_csv_input_fn,\n",
    "        'tfr': make_tfr_input_fn\n",
    "    }\n",
    "write_py(make_input_fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Read from CSV\n",
    "Not so good for production, but may come handy for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_csv_input_fn written to ./train/make_csv_input_fn.py.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_csv_input_fn(filename_pattern, batch_size, options): \n",
    "# batch_size, shuffle_buffer_size=None, distribute=False\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from train.model_config import ORDERED_TRAINING_DEFAULTS\n",
    "    from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "    from train.model_config import LABEL_COLUMN\n",
    "    \n",
    "    \n",
    "    def _input_fn():\n",
    "        filenames = tf.gfile.Glob(filename_pattern)\n",
    "        dataset = tf.data.TextLineDataset(filenames)\n",
    "\n",
    "        def decode_csv(row):\n",
    "            cols = tf.decode_csv(row, record_defaults=ORDERED_TRAINING_DEFAULTS)\n",
    "            features = dict(zip(ORDERED_TRAINING_COLUMNS, cols))\n",
    "            return features\n",
    "\n",
    "        def pop_target(features):\n",
    "            target = features.pop(LABEL_COLUMN)\n",
    "            return features, target\n",
    "        \n",
    "        if options['shuffle_buffer_size'] is not None:\n",
    "            dataset = dataset.shuffle(buffer_size=options['shuffle_buffer_size'])\n",
    "                \n",
    "        dataset = (dataset.repeat()\n",
    "                   .map(decode_csv)\n",
    "                   .map(pop_target)\n",
    "                   .batch(batch_size))\n",
    "        \n",
    "        if options['distribute']:\n",
    "            return dataset \n",
    "        else:\n",
    "            return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_csv_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the input_function's behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=\"samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'AIRLINE': array([5, 5], dtype=int32),\n",
       "   'ARR': array([58, 43], dtype=int32),\n",
       "   'ARR_LAT': array([30.19, 36.12], dtype=float32),\n",
       "   'ARR_LON': array([-97.67, -86.67], dtype=float32),\n",
       "   'DEP_DELAY': array([0.07692308, 0.10204082], dtype=float32),\n",
       "   'DEP_DOW': array([1, 5], dtype=int32),\n",
       "   'DEP_HOD': array([16, 17], dtype=int32),\n",
       "   'DEP_LAT': array([33.63, 33.63], dtype=float32),\n",
       "   'DEP_LON': array([-84.42, -84.42], dtype=float32),\n",
       "   'DIFF_LAT': array([0.2873246 , 0.42374048], dtype=float32),\n",
       "   'DIFF_LON': array([0.6470146, 0.7651418], dtype=float32),\n",
       "   'DISTANCE': array([0.16594191, 0.03058177], dtype=float32),\n",
       "   'MEAN_TEMP_ARR': array([0.7635379 , 0.54512626], dtype=float32),\n",
       "   'MEAN_TEMP_DEP': array([0.91176444, 0.75210077], dtype=float32),\n",
       "   'MEAN_VIS_ARR': array([0.4892473 , 0.45698923], dtype=float32),\n",
       "   'MEAN_VIS_DEP': array([0.70312506, 1.        ], dtype=float32),\n",
       "   'WND_SPD_ARR': array([0.00580058, 0.00330033], dtype=float32),\n",
       "   'WND_SPD_DEP': array([0.49612403, 0.36434108], dtype=float32)},\n",
       "  array([-4., 47.], dtype=float32))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_input_fn = make_input_fns()['csv']\n",
    "with tf.Session() as sess:\n",
    "    train_input_fn = make_input_fn(\n",
    "        \"gs://going-tfx/{}/eval_data/*\".format(DATASET),\n",
    "        options={'batch_size':2,\n",
    "                 'shuffle_buffer_size': None,\n",
    "                 'distribute': False\n",
    "                })\n",
    "    input = train_input_fn()\n",
    "    res = [sess.run(input) for i in range(1)]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Read from TFRecords File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_tfr_input_fn written to ./train/make_tfr_input_fn.py.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tfr_input_fn(filename_pattern, batch_size, options):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from train.model_config import LABEL_COLUMN\n",
    "    from train.model_config import TRAINING_METADATA\n",
    "\n",
    "    feature_spec = TRAINING_METADATA.schema.as_feature_spec()\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=filename_pattern,\n",
    "            batch_size=batch_size,\n",
    "            features=feature_spec,\n",
    "            shuffle_buffer_size=options['shuffle_buffer_size'],\n",
    "            prefetch_buffer_size=options['prefetch_buffer_size'],\n",
    "            reader_num_threads=options['reader_num_threads'],\n",
    "            parser_num_threads=options['parser_num_threads'],\n",
    "            sloppy_ordering=options['sloppy_ordering'],\n",
    "            label_key=LABEL_COLUMN)\n",
    "\n",
    "        if options['distribute']:\n",
    "            return dataset \n",
    "        else:\n",
    "            return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_tfr_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'AIRLINE': array([0, 1]),\n",
       "   'ARR': array([58, 94]),\n",
       "   'ARR_LAT': array([30.19, 28.1 ], dtype=float32),\n",
       "   'ARR_LON': array([-97.67, -80.64], dtype=float32),\n",
       "   'DEP_DELAY': array([0.07692308, 0.07692308], dtype=float32),\n",
       "   'DEP_DOW': array([5, 2]),\n",
       "   'DEP_HOD': array([22, 16]),\n",
       "   'DEP_LAT': array([33.63, 33.63], dtype=float32),\n",
       "   'DEP_LON': array([-84.42, -84.42], dtype=float32),\n",
       "   'DIFF_LAT': array([0.2873246 , 0.23924546], dtype=float32),\n",
       "   'DIFF_LON': array([0.6470146, 0.8298969], dtype=float32),\n",
       "   'DISTANCE': array([0.1659419 , 0.08237782], dtype=float32),\n",
       "   'MEAN_TEMP_ARR': array([0.68592054, 0.59205776], dtype=float32),\n",
       "   'MEAN_TEMP_DEP': array([0.605042  , 0.75210077], dtype=float32),\n",
       "   'MEAN_VIS_ARR': array([0.45161292, 0.3817204 ], dtype=float32),\n",
       "   'MEAN_VIS_DEP': array([0.875     , 0.81250006], dtype=float32),\n",
       "   'WND_SPD_ARR': array([0.00860086, 0.00450045], dtype=float32),\n",
       "   'WND_SPD_DEP': array([0.2868217 , 0.33333334], dtype=float32)},\n",
       "  array([  9., -15.], dtype=float32))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_input_fn = make_input_fns()['tfr']\n",
    "with tf.Session() as sess:\n",
    "    train_input_fn = make_input_fn(\n",
    "        \"gs://going-tfx/{}/train_data/atl_june_tfr-00000-of-*\".format(DATASET), \n",
    "        batch_size=2,                 \n",
    "        options={'shuffle_buffer_size': 10000,\n",
    "                'prefetch_buffer_size': 10000,\n",
    "                'reader_num_threads': 16,\n",
    "                'parser_num_threads': 16,\n",
    "                'sloppy_ordering': True,\n",
    "                'distribute': False})\n",
    "    input = train_input_fn()\n",
    "    res = [sess.run(input) for i in range(1)]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
