{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./prep\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Preprocessing data for ML with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature data in Bigquery\n",
    "We collected the raw data that we use from various sources into a single denormalized table holding the data in so-called signature format. That table's schema is meant to reflect the structure of the data/requests that we expect to be served at prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>AIRLINE_NAME</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>DEP_T</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>...</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "      <th>ARR_T</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>ARR_W</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-06-01</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>US Airways Inc.: US (Merged with America West ...</td>\n",
       "      <td>US</td>\n",
       "      <td>610</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>712</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>CLT</td>\n",
       "      <td>35.21</td>\n",
       "      <td>-80.94</td>\n",
       "      <td>CHARLOTTE/DOUGLAS INTERNATION</td>\n",
       "      <td>78.3</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-06-01</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>620</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>740</td>\n",
       "      <td>9.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>28.42</td>\n",
       "      <td>-81.30</td>\n",
       "      <td>ORLANDO INTERNATIONAL AIRPORT</td>\n",
       "      <td>77.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-06-01</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>620</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>738</td>\n",
       "      <td>55.0</td>\n",
       "      <td>TPA</td>\n",
       "      <td>27.97</td>\n",
       "      <td>-82.53</td>\n",
       "      <td>TAMPA INTERNATIONAL AIRPORT</td>\n",
       "      <td>79.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  YEAR  MONTH  DAY  DEP_DOW  \\\n",
       "0  2002-06-01  2002      6    1        7   \n",
       "1  2002-06-01  2002      6    1        7   \n",
       "2  2002-06-01  2002      6    1        7   \n",
       "\n",
       "                                        AIRLINE_NAME AIRLINE  DEP_T  DEP  \\\n",
       "0  US Airways Inc.: US (Merged with America West ...      US    610  ATL   \n",
       "1                           Delta Air Lines Inc.: DL      DL    620  ATL   \n",
       "2                           Delta Air Lines Inc.: DL      DL    620  ATL   \n",
       "\n",
       "   DEP_LAT     ...       WND_SPD_DEP  ARR_T ARR_DELAY  ARR  ARR_LAT  ARR_LON  \\\n",
       "0    33.63     ...               6.9    712     -12.0  CLT    35.21   -80.94   \n",
       "1    33.63     ...               6.9    740       9.0  MCO    28.42   -81.30   \n",
       "2    33.63     ...               6.9    738      55.0  TPA    27.97   -82.53   \n",
       "\n",
       "                           ARR_W  MEAN_TEMP_ARR MEAN_VIS_ARR  WND_SPD_ARR  \n",
       "0  CHARLOTTE/DOUGLAS INTERNATION           78.3          9.5          2.7  \n",
       "1  ORLANDO INTERNATIONAL AIRPORT           77.4          9.6          5.7  \n",
       "2    TAMPA INTERNATIONAL AIRPORT           79.1          9.9          5.5  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery sample\n",
    "select * FROM `going-tfx.examples.ATL_JUNE_SIGNATURE` limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEP_DOW', 'DEP_T', 'DEP_LAT', 'DEP_LON', 'DEP_DELAY', 'MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'ARR_LAT', 'ARR_LON', 'ARR_DELAY', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR', 'ARR', 'AIRLINE']\n"
     ]
    }
   ],
   "source": [
    "from train.model_config import SIGNATURE_COLUMNS\n",
    "print(SIGNATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_queries written to ./prep/sample_queries.py.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_queries(columns, fractions, rate=0.1):\n",
    "\n",
    "    def sample_query(columns, total, lower, upper):\n",
    "        col_string=\", \".join(columns)\n",
    "        return \"\"\"\n",
    "        SELECT\n",
    "            {0}\n",
    "        FROM \n",
    "            `going-tfx.examples.ATL_JUNE_SIGNATURE` \n",
    "        where\n",
    "            MOD(ABS(FARM_FINGERPRINT(\n",
    "                CONCAT(DATE,AIRLINE,ARR)\n",
    "            )) + DEP_T, {1}) >= {2} \n",
    "        and\n",
    "            MOD(ABS(FARM_FINGERPRINT(\n",
    "                CONCAT( DATE, AIRLINE, ARR)\n",
    "            )) + DEP_T, {1}) < {3} \n",
    "        \"\"\".format(col_string, total, lower, upper)\n",
    "    \n",
    "    start = 0\n",
    "    total = int(sum(fractions) / rate)\n",
    "    res = []\n",
    "    for f in fractions:\n",
    "        f_ = int(f) \n",
    "        q = sample_query(columns, total, start, start+f_)\n",
    "        start = start + f_\n",
    "        res.append(q)\n",
    "    return dict(zip(['train', 'eval', 'test'], res))\n",
    "write_py(sample_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 2661 examples. Showing first three:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_T</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>ARR</th>\n",
       "      <th>AIRLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1509</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>39.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>11.1</td>\n",
       "      <td>40.65</td>\n",
       "      <td>-75.44</td>\n",
       "      <td>24.0</td>\n",
       "      <td>78.5</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>ABE</td>\n",
       "      <td>EV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2205</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>204.0</td>\n",
       "      <td>85.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>40.65</td>\n",
       "      <td>-75.44</td>\n",
       "      <td>185.0</td>\n",
       "      <td>72.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ABE</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1050</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>35.04</td>\n",
       "      <td>-106.60</td>\n",
       "      <td>11.0</td>\n",
       "      <td>74.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEP_DOW  DEP_T  DEP_LAT  DEP_LON  DEP_DELAY  MEAN_TEMP_DEP  MEAN_VIS_DEP  \\\n",
       "0        1   1509    33.63   -84.42       39.0           76.0           7.5   \n",
       "1        3   2205    33.63   -84.42      204.0           85.3          10.0   \n",
       "2        1   1050    33.63   -84.42        9.0           75.7           9.3   \n",
       "\n",
       "   WND_SPD_DEP  ARR_LAT  ARR_LON  ARR_DELAY  MEAN_TEMP_ARR  MEAN_VIS_ARR  \\\n",
       "0         11.1    40.65   -75.44       24.0           78.5           9.8   \n",
       "1          5.1    40.65   -75.44      185.0           72.3          10.0   \n",
       "2          6.5    35.04  -106.60       11.0           74.1          10.0   \n",
       "\n",
       "   WND_SPD_ARR  ARR AIRLINE  \n",
       "0          5.5  ABE      EV  \n",
       "1          4.0  ABE      FL  \n",
       "2         12.4  ABQ      DL  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.datalab.bigquery as dlbq\n",
    "queries = sample_queries(SIGNATURE_COLUMNS, [90,5,5], .01)\n",
    "df = dlbq.Query(queries['train']).execute().result().to_dataframe()\n",
    "print('Only {} examples. Showing first three:'.format(len(df)))\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### The pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre_process written to ./prep/pre_process.py.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_process(row):\n",
    "    import tensorflow_transform as tft\n",
    "    from tools import tf_haversine\n",
    "\n",
    "    def add_engineered(row):\n",
    "        dep_lat = row['DEP_LAT']\n",
    "        dep_lon = row['DEP_LON']\n",
    "        arr_lat = row['ARR_LAT']\n",
    "        arr_lon = row['ARR_LON']\n",
    "\n",
    "        row['DEP_HOD'] = row['DEP_T'] // 100\n",
    "        row.pop('DEP_T')  # no longer needed\n",
    "\n",
    "        row['DIFF_LAT'] = arr_lat - dep_lat\n",
    "        row['DIFF_LON'] = arr_lon - dep_lon\n",
    "        row['DISTANCE'] = tf_haversine(arr_lat, arr_lon, dep_lat, dep_lon)\n",
    "        return row\n",
    "\n",
    "    def scale_floats(row):\n",
    "        for c in ['MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR', 'DEP_DELAY',\n",
    "                 'DIFF_LAT', 'DIFF_LON', 'DISTANCE']:\n",
    "            row[c] = tft.scale_to_0_1(row[c])\n",
    "        return row\n",
    "\n",
    "    def categorical_from_strings(row):\n",
    "        row['AIRLINE'] = tft.string_to_int(row['AIRLINE'])\n",
    "        row['ARR'] = tft.string_to_int(row['ARR'])\n",
    "        return row\n",
    "    \n",
    "    row = row.copy()\n",
    "    row = add_engineered(row)\n",
    "    row = scale_floats(row)\n",
    "    row = categorical_from_strings(row)\n",
    "    return row\n",
    "write_py(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEP_DOW', 'DEP_HOD', 'AIRLINE', 'ARR', 'DEP_LAT', 'DEP_LON', 'MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'DEP_DELAY', 'ARR_LAT', 'ARR_LON', 'ARR_DELAY', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR', 'DIFF_LAT', 'DIFF_LON', 'DISTANCE']\n"
     ]
    }
   ],
   "source": [
    "from train.model_config import TRAINING_COLUMNS\n",
    "print(TRAINING_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### The full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exec_pipeline_prod written to ./prep/exec_pipeline_prod.py.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exec_pipeline_prod (options, train_dir, eval_dir, test_dir, \n",
    "                        metadata_dir, tmp_dir, \n",
    "                        fractions, sample_rate, prefix, \n",
    "                        encode='tfrecord', \n",
    "                        runner='DirectRunner'):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_transform as tft\n",
    "    import tensorflow_transform.beam.impl as beam_impl\n",
    "    import apache_beam as beam\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "    \n",
    "    from train.model_config import (SIGNATURE_COLUMNS, TRAINING_COLUMNS,\n",
    "        TRAINING_METADATA, SIGNATURE_METADATA, ORDERED_TRAINING_COLUMNS)\n",
    "    from prep.pre_process import pre_process\n",
    "    from prep.sample_queries import sample_queries\n",
    "\n",
    "    with beam.Pipeline(runner, options=options) as p:\n",
    "        with beam_impl.Context(temp_dir=tmp_dir):\n",
    "\n",
    "            def write_to_files(data, prefix, phase):\n",
    "                tfr_encoder = tft.coders.ExampleProtoCoder(t_metadata.schema)            \n",
    "                if encode in ['tfrecord', 'both', None]:\n",
    "                    _ = (data\n",
    "                        | ('EncodeTFRecord_' + phase) >> beam.Map(tfr_encoder.encode)\n",
    "                        | ('WriteTFRecord_' + phase) >> beam.io.WriteToTFRecord(prefix+'_tfr'))\n",
    "\n",
    "                if encode in ['csv', 'both', None]:\n",
    "                    csv_encoder = tft.coders.CsvCoder(ORDERED_TRAINING_COLUMNS, TRAINING_METADATA.schema)    \n",
    "                    _ = (data \n",
    "                        | ('EncodeCSV_train' + phase) >> beam.Map(csv_encoder.encode)\n",
    "                        | ('WriteText_train' + phase) >> beam.io.WriteToText(file_path_prefix=prefix+'_csv'))\n",
    "        \n",
    "            # Process training data and obtain transform_fn\n",
    "            #\n",
    "            queries = sample_queries(SIGNATURE_COLUMNS, fractions, sample_rate)\n",
    "\n",
    "            signature_data = (p | \"ReadFromBigQuery_train\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['train'], use_standard_sql=True)))\n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "            \n",
    "            tds, transform_fn = (signature_dataset | \"AnalyzeAndTransform\" \n",
    "                        >> beam_impl.AnalyzeAndTransformDataset(pre_process))\n",
    "            t_data, t_metadata = tds\n",
    "\n",
    "            train_prefix = os.path.join(train_dir, prefix)\n",
    "            write_to_files(t_data, train_prefix, 'train')\n",
    "            \n",
    "            #  Process evaluation data with the obtained transform_fn\n",
    "            #\n",
    "            signature_data = (p | \"ReadFromBigQuery_eval\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['eval'], use_standard_sql=True))) \n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "\n",
    "            t_dataset = ((signature_dataset, transform_fn) \n",
    "                         | \"TransformEval\" >> beam_impl.TransformDataset())\n",
    "            t_data, t_metadata = t_dataset\n",
    "\n",
    "            eval_prefix = os.path.join(eval_dir, prefix)\n",
    "            write_to_files(t_data, eval_prefix, 'eval')\n",
    "\n",
    "            #  Also process test data with the obtained transform_fn\n",
    "            #\n",
    "            signature_data = (p | \"ReadFromBigQuery_test\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['test'], use_standard_sql=True)))\n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "\n",
    "            t_dataset = ((signature_dataset, transform_fn) \n",
    "                         | \"TransformTest\" >> beam_impl.TransformDataset())\n",
    "            t_data, t_metadata = t_dataset           \n",
    "\n",
    "            test_prefix = os.path.join(test_dir, prefix)\n",
    "            write_to_files(t_data, test_prefix, 'text')\n",
    "            \n",
    "            # save transforma function to disk for use at serving time\n",
    "            #\n",
    "            transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(metadata_dir)\n",
    "\n",
    "write_py(exec_pipeline_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run_job written to ./prep/run_job.py.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_job(args):\n",
    "    \n",
    "    import datetime\n",
    "    import apache_beam as beam\n",
    "    from prep.exec_pipeline_prod import exec_pipeline_prod\n",
    "    \n",
    "    job_name = 'tft-tutorial' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "    \n",
    "    options = {\n",
    "        'staging_location': args['stage_dir'],\n",
    "        'temp_location': args['tmp_dir'],\n",
    "        'job_name': job_name,\n",
    "        'project': args['project'],\n",
    "        'max_num_workers': int(args['max_workers']),\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'requirements_file': 'dataflow_requirements.txt'\n",
    "    }    \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "    fractions = [int(n) for n in args['fractions'].split(\",\")]\n",
    "\n",
    "    exec_pipeline_prod (opts, args['train_dir'], args['eval_dir'],args['test_dir'],\n",
    "                        args['metadata_dir'], args['tmp_dir'],\n",
    "                        fractions, float(args['sample_rate']), args['prefix'],\n",
    "                        encode=args['encode'], runner=args['runner'])\n",
    "write_py(run_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(subproject):\n",
    "    import os\n",
    "    basedir = os.path.join('gs://going-tfx/', subproject)\n",
    "\n",
    "    for d in ['train_data/*', 'eval_data/*', 'test_data/*', 'tmp/*', 'model/*', 'metadata/*']:\n",
    "        target = os.path.join(basedir, d)\n",
    "        !echo gsutil -m rm -rf $target\n",
    "        _ = !gsutil -m rm -rf $target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET='samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -m rm -rf gs://going-tfx/samples/train_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/samples/eval_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/samples/test_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/samples/tmp/*\n",
      "gsutil -m rm -rf gs://going-tfx/samples/model/*\n",
      "gsutil -m rm -rf gs://going-tfx/samples/metadata/*\n"
     ]
    }
   ],
   "source": [
    "cleanup(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/tmp/tftransform_tmp/71470af6999e4d9ab97a29f4cb6d3c60/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/tmp/tftransform_tmp/71470af6999e4d9ab97a29f4cb6d3c60/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/tmp/tftransform_tmp/2a6f8963ab0c46dc864262dea93710b1/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/tmp/tftransform_tmp/2a6f8963ab0c46dc864262dea93710b1/saved_model.pb\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_a00cacec892d44c680981c56a52b6571 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_4ca0b28e58c24c45a9678f7a43920015 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_73e64fb63b494b63bc68021edeaf8af4 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://going-tfx/samples/tmp/tftransform_tmp/f410d61a5bd9452588f77c6f4aceb110/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://going-tfx/samples/tmp/tftransform_tmp/f410d61a5bd9452588f77c6f4aceb110/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/tmp/tftransform_tmp/f410d61a5bd9452588f77c6f4aceb110/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://going-tfx/samples/tmp/tftransform_tmp/f410d61a5bd9452588f77c6f4aceb110/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    }
   ],
   "source": [
    "from prep.prep_tools import join_paths\n",
    "\n",
    "args={}\n",
    "args['base_dir'] = \"gs://going-tfx/{}\".format(DATASET)\n",
    "args['train_dir'] = 'train_data'\n",
    "args['eval_dir'] = 'eval_data'\n",
    "args['test_dir'] = 'test_data'\n",
    "args['metadata_dir'] = 'metadata'\n",
    "args['stage_dir'] = 'staging'\n",
    "args['tmp_dir'] = 'tmp'\n",
    "args['project'] = 'going-tfx'\n",
    "args['prefix'] = 'atl_june'\n",
    "args['fractions'] = '80,10,10'\n",
    "args['sample_rate'] = 0.1\n",
    "args['max_workers'] = 10\n",
    "args['runner'] = 'DirectRunner'\n",
    "args['encode'] = 'both'\n",
    "\n",
    "\n",
    "run_job(join_paths(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_gs(gsglob):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "\n",
    "    a_training_file = !gsutil ls $gsglob\n",
    "    a_training_file = a_training_file[0]\n",
    "    TEMP_DIR='/tmp/atl_june/{}'.format(DATASET)\n",
    "    !mkdir -p $TEMP_DIR\n",
    "    _ = !gsutil cp $a_training_file $TEMP_DIR\n",
    "    a_training_file = !ls $TEMP_DIR\n",
    "    a_training_file = os.path.join(TEMP_DIR,a_training_file[0])\n",
    "    res=!wc -l $a_training_file\n",
    "    res=res[0].split(\" \")\n",
    "    print()\n",
    "    print(\"{} records in {}\".format(res[0], res[1]))\n",
    "    return pd.read_csv(a_training_file, names=ORDERED_TRAINING_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 records in /tmp/atl_june/samples/atl_june_csv-00000-of-00024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_HOD</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DIFF_LAT</th>\n",
       "      <th>DIFF_LON</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>40.69</td>\n",
       "      <td>-74.16</td>\n",
       "      <td>0.094192</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.52887</td>\n",
       "      <td>0.899484</td>\n",
       "      <td>0.150884</td>\n",
       "      <td>0.490975</td>\n",
       "      <td>0.592437</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>37.0</td>\n",
       "      <td>42.94</td>\n",
       "      <td>-87.89</td>\n",
       "      <td>0.135008</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.58063</td>\n",
       "      <td>0.752040</td>\n",
       "      <td>0.133809</td>\n",
       "      <td>0.496390</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.279070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AIRLINE  ARR  ARR_DELAY  ARR_LAT  ARR_LON  DEP_DELAY  DEP_DOW  DEP_HOD  \\\n",
       "296        0    3       -5.0    40.69   -74.16   0.094192        4       12   \n",
       "859        0   29       37.0    42.94   -87.89   0.135008        3       20   \n",
       "\n",
       "     DEP_LAT  DEP_LON  DIFF_LAT  DIFF_LON  DISTANCE  MEAN_TEMP_ARR  \\\n",
       "296    33.63   -84.42   0.52887  0.899484  0.150884       0.490975   \n",
       "859    33.63   -84.42   0.58063  0.752040  0.133809       0.496390   \n",
       "\n",
       "     MEAN_TEMP_DEP  MEAN_VIS_ARR  MEAN_VIS_DEP  WND_SPD_ARR  WND_SPD_DEP  \n",
       "296       0.592437      0.489247      0.734375     0.010801     0.263566  \n",
       "859       0.277311      0.489247      0.609375     0.007901     0.279070  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_from_gs('gs://going-tfx/$DATASET/train_data/atl_june_csv-00000-of-*')\n",
    "probe.sample(frac=1.0)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Make below cell a code cell and execute it within the notebook or better execute this code on a terminal. It's going to take up to 20 minutes, if sample_rate is 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
    "python -m prep.task \\\n",
    "    --project=going-tfx \\\n",
    "    --base_dir=gs://going-tfx/$DATASET/ \\\n",
    "    --sample_rate=1.0 \\\n",
    "    --prefix=atl_june \\\n",
    "    --encode=tfrecord \\\n",
    "    --runner=DataflowRunner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
